{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0m8YROdDpRAk",
    "outputId": "e6bb48eb-1c2b-4a99-9558-be452345ba23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch Version 1.5.0+cu101\n"
     ]
    }
   ],
   "source": [
    "import math_dataset \n",
    "from math_dataset import MathDatasetManager\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "from math_dataset import (\n",
    "    question_answer_to_position_batch_collate_fn\n",
    ")\n",
    "import model_process\n",
    "\n",
    "\n",
    "import utils\n",
    "\n",
    "%matplotlib notebook  \n",
    "\n",
    "print(\"Torch Version\", torch.__version__)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Math Dataset Manager\n",
    "\n",
    "This class is just a Numpy/Pytorch helper to manage all files in Math Dataset v1.0 and select different parts of it by categories or modules to generate a Pytorch dataset for training. Pytorch Datasets created doesn't mount all questions/answers in memory and use Pandas limited streaming features to bufferize data. It allows loading huge files quite fast while keeping memory print reasonable. It also caches lazy datasets and allows fast re-using previously created ones.\n",
    "\n",
    "Here are the main features provided right now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Math Dataset Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized MultiFilesMathDataset with categories ['algebra', 'arithmetic', 'calculus', 'comparison', 'measurement', 'numbers', 'polynomials', 'probability'] and types ['train-easy', 'train-medium', 'train-hard', 'interpolate', 'extrapolate']\n"
     ]
    }
   ],
   "source": [
    "mdsmgr = MathDatasetManager(\n",
    "  \"C:\\\\Users\\\\Jesús\\\\Documents\\\\PC2\\\\TorchDemo\\\\hs-math-nlp\\\\mathematics_dataset-v1.0\\\\mathematics_dataset-v1.0\\\\\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check availables types (difficulties + interpolate + extrapolate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "types ['train-easy', 'train-medium', 'train-hard', 'interpolate', 'extrapolate']\n"
     ]
    }
   ],
   "source": [
    "print(\"types\", list(mdsmgr.get_types()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check availables problem categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categories ['algebra', 'arithmetic', 'calculus', 'comparison', 'measurement', 'numbers', 'polynomials', 'probability']\n"
     ]
    }
   ],
   "source": [
    "print(\"categories\", list(mdsmgr.get_categories()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device cuda\n"
     ]
    }
   ],
   "source": [
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "device = torch.device(\"cuda\")\n",
    "print(\"device\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on Algebra Linear_1d in Easy mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an experiment with a name and a unique ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = \"add_or_sub\" # \"math_ds_algebra_linear_1d_easy\"\n",
    "unique_id = \"2021-07-25\" # \"2019-05-25_0900\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Dataset for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-easy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-easy dataset size 666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jesús\\Documents\\PC2\\TorchDemo\\hs-math-nlp\\torch\\lib\\site-packages\\pandas\\core\\frame.py:5042: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    }
   ],
   "source": [
    "ds = mdsmgr.build_dataset_from_module(\n",
    "    'arithmetic', 'add_or_sub', 'train-hard'\n",
    ")\n",
    "print(\"train-easy dataset size\", len(ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpolate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interpolate dataset size 10000\n"
     ]
    }
   ],
   "source": [
    "ds_interpolate = mdsmgr.build_dataset_from_module(\n",
    "    'arithmetic', 'add_or_sub', 'interpolate'\n",
    ")\n",
    "print(\"interpolate dataset size\", len(ds_interpolate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create default Transformer model\n",
    "\n",
    "Here we test the best model found in the paper: a multi-head self-attention transformer to give a default sample.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = utils.build_transformer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create basic optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=6e-6, betas=(0.9, 0.995), eps=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Pytorch dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we split data in 90/10% for train/validation and use interpolate for test\n",
    "train_ds, val_ds = math_dataset.random_split_dataset(ds, split_rate=0.9)\n",
    "\n",
    "# we provide the function question_answer_to_position_batch_collate_fn that collates\n",
    "# all questions/answers into transformer format enhanced with char positioning\n",
    "train_loader = data.DataLoader(\n",
    "    train_ds, batch_size=128, shuffle=True, num_workers=12,\n",
    "    collate_fn=question_answer_to_position_batch_collate_fn)\n",
    "\n",
    "val_loader = data.DataLoader(\n",
    "    val_ds, batch_size=128, shuffle=False, num_workers=12,\n",
    "    collate_fn=question_answer_to_position_batch_collate_fn)\n",
    "\n",
    "interpolate_loader = data.DataLoader(\n",
    "    ds_interpolate, batch_size=128, shuffle=False, num_workers=12,\n",
    "    collate_fn=question_answer_to_position_batch_collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import checkpoints\n",
    "\n",
    "\n",
    "# build default transformer model\n",
    "#model = utils.build_transformer()\n",
    "\n",
    "#model_exp_name = \"linear_algebra\" # \"math_ds_algebra_linear_1d_easy\"\n",
    "#model_unique_id  = \"2020-07-22\" # \"2019-05-25_0900\"\n",
    "#model_exp_name = 'math_ds_algebra_linear_1d_easy'\n",
    "#model_unique_id = '2019-10-27_2300'\n",
    "# restore best validation model from checkpoint\n",
    "#_ = checkpoints.restore_checkpoint(\".\\\\checkpoints\\\\checkpoint_b37504_e7.pth\",\"\", model=model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~ Beginning Training ~~~~\n",
      "Start epoch: 0, Start batch: 0, Max batch: None\n",
      "[ Epoch: 0 / 8, Run Batch: 0 / None]\n",
      "Batch: 0. Acc: 0.011730. Loss: 5.227549. Batch_acc: 0.011730. Batch_loss: 5.227549 \n",
      "Batch: 1. Acc: 0.019573. Loss: 5.078589. Batch_acc: 0.027357. Batch_loss: 4.930756 \n",
      "Batch: 2. Acc: 0.036892. Loss: 4.888388. Batch_acc: 0.071765. Batch_loss: 4.505412 \n",
      "Batch: 3. Acc: 0.048684. Loss: 4.726118. Batch_acc: 0.083867. Batch_loss: 4.241954 \n",
      "Batch: 4. Acc: 0.055477. Loss: 4.584731. Batch_acc: 0.083037. Batch_loss: 4.011135 \n",
      "Batch: 5. Acc: 0.061344. Loss: 4.453219. Batch_acc: 0.090855. Batch_loss: 3.791699 \n",
      "Batch: 6. Acc: 0.064137. Loss: 4.332433. Batch_acc: 0.080271. Batch_loss: 3.634551 \n",
      "Batch: 7. Acc: 0.066735. Loss: 4.224350. Batch_acc: 0.084677. Batch_loss: 3.477855 \n",
      "Batch: 8. Acc: 0.069298. Loss: 4.123463. Batch_acc: 0.089647. Batch_loss: 3.322553 \n",
      "Batch: 9. Acc: 0.072976. Loss: 4.033534. Batch_acc: 0.106930. Batch_loss: 3.203272 \n",
      "Batch: 10. Acc: 0.075828. Loss: 3.952317. Batch_acc: 0.104734. Batch_loss: 3.129143 \n",
      "Batch: 11. Acc: 0.077447. Loss: 3.876167. Batch_acc: 0.094991. Batch_loss: 3.051150 \n",
      "Batch: 12. Acc: 0.078354. Loss: 3.805161. Batch_acc: 0.088990. Batch_loss: 2.972525 \n",
      "Batch: 13. Acc: 0.081757. Loss: 3.739774. Batch_acc: 0.125432. Batch_loss: 2.900469 \n",
      "Batch: 14. Acc: 0.081948. Loss: 3.682162. Batch_acc: 0.084571. Batch_loss: 2.890501 \n",
      "Batch: 15. Acc: 0.083040. Loss: 3.627576. Batch_acc: 0.099089. Batch_loss: 2.825670 \n",
      "Batch: 16. Acc: 0.084022. Loss: 3.578828. Batch_acc: 0.099597. Batch_loss: 2.805567 \n",
      "Batch: 17. Acc: 0.084876. Loss: 3.532390. Batch_acc: 0.099038. Batch_loss: 2.762635 \n",
      "Batch: 18. Acc: 0.085876. Loss: 3.490668. Batch_acc: 0.103889. Batch_loss: 2.738616 \n",
      "Batch: 19. Acc: 0.086915. Loss: 3.451001. Batch_acc: 0.106155. Batch_loss: 2.716803 \n",
      "Batch: 20. Acc: 0.087417. Loss: 3.414339. Batch_acc: 0.097230. Batch_loss: 2.698283 \n",
      "Batch: 21. Acc: 0.088043. Loss: 3.380179. Batch_acc: 0.100845. Batch_loss: 2.681187 \n",
      "Batch: 22. Acc: 0.088856. Loss: 3.349166. Batch_acc: 0.106955. Batch_loss: 2.658670 \n",
      "Batch: 23. Acc: 0.090144. Loss: 3.319563. Batch_acc: 0.119074. Batch_loss: 2.654581 \n",
      "Batch: 24. Acc: 0.090844. Loss: 3.291886. Batch_acc: 0.107183. Batch_loss: 2.646111 \n",
      "Batch: 25. Acc: 0.091328. Loss: 3.266957. Batch_acc: 0.103389. Batch_loss: 2.646103 \n",
      "Batch: 26. Acc: 0.092094. Loss: 3.243564. Batch_acc: 0.112281. Batch_loss: 2.626559 \n",
      "Batch: 27. Acc: 0.093487. Loss: 3.220639. Batch_acc: 0.131395. Batch_loss: 2.596725 \n",
      "Batch: 28. Acc: 0.094933. Loss: 3.197990. Batch_acc: 0.133963. Batch_loss: 2.587010 \n",
      "Batch: 29. Acc: 0.096036. Loss: 3.177657. Batch_acc: 0.128430. Batch_loss: 2.580240 \n",
      "Batch: 30. Acc: 0.097037. Loss: 3.158353. Batch_acc: 0.126785. Batch_loss: 2.584594 \n",
      "Batch: 31. Acc: 0.097757. Loss: 3.140400. Batch_acc: 0.120279. Batch_loss: 2.579252 \n",
      "Batch: 32. Acc: 0.098004. Loss: 3.123772. Batch_acc: 0.105869. Batch_loss: 2.592647 \n",
      "Batch: 33. Acc: 0.098646. Loss: 3.107148. Batch_acc: 0.119590. Batch_loss: 2.565127 \n",
      "Batch: 34. Acc: 0.099554. Loss: 3.091708. Batch_acc: 0.130662. Batch_loss: 2.562596 \n",
      "Batch: 35. Acc: 0.100639. Loss: 3.076741. Batch_acc: 0.138506. Batch_loss: 2.554373 \n",
      "Batch: 36. Acc: 0.101736. Loss: 3.062910. Batch_acc: 0.141691. Batch_loss: 2.559099 \n",
      "Batch: 37. Acc: 0.102715. Loss: 3.048988. Batch_acc: 0.137758. Batch_loss: 2.550601 \n",
      "Batch: 38. Acc: 0.103309. Loss: 3.036225. Batch_acc: 0.125495. Batch_loss: 2.560181 \n",
      "Batch: 39. Acc: 0.103924. Loss: 3.024493. Batch_acc: 0.128462. Batch_loss: 2.556134 \n",
      "Batch: 40. Acc: 0.104699. Loss: 3.013356. Batch_acc: 0.137243. Batch_loss: 2.545779 \n",
      "Batch: 41. Acc: 0.105413. Loss: 3.001610. Batch_acc: 0.133407. Batch_loss: 2.541222 \n",
      "Batch: 42. Acc: 0.106336. Loss: 2.990642. Batch_acc: 0.145078. Batch_loss: 2.530226 \n",
      "Batch: 43. Acc: 0.107185. Loss: 2.980021. Batch_acc: 0.143596. Batch_loss: 2.524638 \n",
      "Batch: 44. Acc: 0.108116. Loss: 2.969856. Batch_acc: 0.148827. Batch_loss: 2.525367 \n",
      "Batch: 45. Acc: 0.108952. Loss: 2.960121. Batch_acc: 0.146651. Batch_loss: 2.520941 \n",
      "Batch: 46. Acc: 0.109554. Loss: 2.951019. Batch_acc: 0.136986. Batch_loss: 2.536069 \n",
      "Batch: 47. Acc: 0.110123. Loss: 2.941847. Batch_acc: 0.136134. Batch_loss: 2.522453 \n",
      "Batch: 48. Acc: 0.110722. Loss: 2.933410. Batch_acc: 0.139710. Batch_loss: 2.525460 \n",
      "Batch: 49. Acc: 0.111219. Loss: 2.925583. Batch_acc: 0.136123. Batch_loss: 2.532930 \n",
      "Batch: 50. Acc: 0.111687. Loss: 2.917751. Batch_acc: 0.135435. Batch_loss: 2.520740 \n",
      "Batch: 51. Acc: 0.112295. Loss: 2.910062. Batch_acc: 0.143436. Batch_loss: 2.516305 \n",
      "Batch: 52. Acc: 0.112756. Loss: 2.902498. Batch_acc: 0.136185. Batch_loss: 2.518282 \n",
      "Batch: 53. Acc: 0.113246. Loss: 2.895576. Batch_acc: 0.139454. Batch_loss: 2.525326 \n",
      "Batch: 54. Acc: 0.113688. Loss: 2.888787. Batch_acc: 0.137791. Batch_loss: 2.518690 \n",
      "Batch: 55. Acc: 0.114302. Loss: 2.882056. Batch_acc: 0.148234. Batch_loss: 2.509855 \n",
      "Batch: 56. Acc: 0.114916. Loss: 2.875134. Batch_acc: 0.148779. Batch_loss: 2.493052 \n",
      "Batch: 57. Acc: 0.115499. Loss: 2.868593. Batch_acc: 0.148487. Batch_loss: 2.498854 \n",
      "Batch: 58. Acc: 0.116090. Loss: 2.862444. Batch_acc: 0.150229. Batch_loss: 2.507287 \n",
      "Batch: 59. Acc: 0.116584. Loss: 2.856376. Batch_acc: 0.145465. Batch_loss: 2.501632 \n",
      "Batch: 60. Acc: 0.117069. Loss: 2.850794. Batch_acc: 0.146714. Batch_loss: 2.509421 \n",
      "Batch: 61. Acc: 0.117759. Loss: 2.845268. Batch_acc: 0.159931. Batch_loss: 2.507264 \n",
      "Batch: 62. Acc: 0.118096. Loss: 2.840061. Batch_acc: 0.139344. Batch_loss: 2.511872 \n",
      "Batch: 63. Acc: 0.118701. Loss: 2.834664. Batch_acc: 0.156863. Batch_loss: 2.494231 \n",
      "Batch: 64. Acc: 0.119190. Loss: 2.829447. Batch_acc: 0.150519. Batch_loss: 2.495167 \n",
      "Batch: 65. Acc: 0.119524. Loss: 2.824431. Batch_acc: 0.141292. Batch_loss: 2.498072 \n",
      "Batch: 66. Acc: 0.120044. Loss: 2.819462. Batch_acc: 0.154514. Batch_loss: 2.489979 \n",
      "Batch: 67. Acc: 0.120409. Loss: 2.814670. Batch_acc: 0.144828. Batch_loss: 2.494402 \n",
      "Batch: 68. Acc: 0.120885. Loss: 2.809858. Batch_acc: 0.153356. Batch_loss: 2.481124 \n",
      "Batch: 69. Acc: 0.121065. Loss: 2.805313. Batch_acc: 0.133643. Batch_loss: 2.489035 \n",
      "Batch: 70. Acc: 0.121609. Loss: 2.800740. Batch_acc: 0.159815. Batch_loss: 2.479002 \n",
      "Batch: 71. Acc: 0.122100. Loss: 2.796305. Batch_acc: 0.157044. Batch_loss: 2.480841 \n",
      "Batch: 72. Acc: 0.122395. Loss: 2.792324. Batch_acc: 0.143944. Batch_loss: 2.501245 \n",
      "Batch: 73. Acc: 0.122874. Loss: 2.787815. Batch_acc: 0.156721. Batch_loss: 2.469330 \n",
      "Batch: 74. Acc: 0.123187. Loss: 2.783825. Batch_acc: 0.146597. Batch_loss: 2.485631 \n",
      "Batch: 75. Acc: 0.123634. Loss: 2.779584. Batch_acc: 0.156355. Batch_loss: 2.469099 \n",
      "Batch: 76. Acc: 0.124180. Loss: 2.775501. Batch_acc: 0.165800. Batch_loss: 2.464314 \n",
      "Batch: 77. Acc: 0.124564. Loss: 2.771830. Batch_acc: 0.154657. Batch_loss: 2.484356 \n",
      "Batch: 78. Acc: 0.125065. Loss: 2.767643. Batch_acc: 0.162356. Batch_loss: 2.455650 \n",
      "Batch: 79. Acc: 0.125318. Loss: 2.764091. Batch_acc: 0.145042. Batch_loss: 2.488014 \n",
      "Batch: 80. Acc: 0.125609. Loss: 2.760644. Batch_acc: 0.148851. Batch_loss: 2.485282 \n",
      "Batch: 81. Acc: 0.125856. Loss: 2.757273. Batch_acc: 0.145809. Batch_loss: 2.485020 \n",
      "Batch: 82. Acc: 0.126046. Loss: 2.753916. Batch_acc: 0.141696. Batch_loss: 2.476237 \n",
      "Batch: 83. Acc: 0.126339. Loss: 2.750676. Batch_acc: 0.151390. Batch_loss: 2.474414 \n",
      "Batch: 84. Acc: 0.126796. Loss: 2.747348. Batch_acc: 0.165598. Batch_loss: 2.464290 \n",
      "Batch: 85. Acc: 0.127101. Loss: 2.744158. Batch_acc: 0.152794. Batch_loss: 2.475795 \n",
      "Batch: 86. Acc: 0.127478. Loss: 2.740698. Batch_acc: 0.159015. Batch_loss: 2.451415 \n",
      "Batch: 87. Acc: 0.127870. Loss: 2.737533. Batch_acc: 0.161960. Batch_loss: 2.461869 \n",
      "Batch: 88. Acc: 0.128149. Loss: 2.734461. Batch_acc: 0.152738. Batch_loss: 2.463787 \n",
      "Batch: 89. Acc: 0.128454. Loss: 2.731576. Batch_acc: 0.155568. Batch_loss: 2.475598 \n",
      "Batch: 90. Acc: 0.128699. Loss: 2.728528. Batch_acc: 0.151142. Batch_loss: 2.449399 \n",
      "Batch: 91. Acc: 0.129046. Loss: 2.725570. Batch_acc: 0.159910. Batch_loss: 2.462351 \n",
      "Batch: 92. Acc: 0.129347. Loss: 2.722784. Batch_acc: 0.157284. Batch_loss: 2.464347 \n",
      "Batch: 93. Acc: 0.129593. Loss: 2.719877. Batch_acc: 0.151941. Batch_loss: 2.455626 \n",
      "Batch: 94. Acc: 0.129825. Loss: 2.717127. Batch_acc: 0.151995. Batch_loss: 2.453582 \n",
      "Batch: 95. Acc: 0.130110. Loss: 2.714246. Batch_acc: 0.157110. Batch_loss: 2.441609 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 96. Acc: 0.130339. Loss: 2.711562. Batch_acc: 0.152632. Batch_loss: 2.449801 \n",
      "Batch: 97. Acc: 0.130475. Loss: 2.708751. Batch_acc: 0.143503. Batch_loss: 2.441198 \n",
      "Batch: 98. Acc: 0.130731. Loss: 2.706103. Batch_acc: 0.155280. Batch_loss: 2.451573 \n",
      "Batch: 99. Acc: 0.130877. Loss: 2.703791. Batch_acc: 0.145821. Batch_loss: 2.468104 \n",
      "Batch: 100. Acc: 0.131144. Loss: 2.701082. Batch_acc: 0.156983. Batch_loss: 2.438200 \n",
      "Batch: 101. Acc: 0.131427. Loss: 2.698730. Batch_acc: 0.160279. Batch_loss: 2.459063 \n",
      "Batch: 102. Acc: 0.131773. Loss: 2.696201. Batch_acc: 0.167245. Batch_loss: 2.436856 \n",
      "Batch: 103. Acc: 0.132127. Loss: 2.693534. Batch_acc: 0.168682. Batch_loss: 2.418709 \n",
      "Batch: 104. Acc: 0.132377. Loss: 2.690947. Batch_acc: 0.158537. Batch_loss: 2.419548 \n",
      "Batch: 105. Acc: 0.132619. Loss: 2.688648. Batch_acc: 0.158107. Batch_loss: 2.446695 \n",
      "Batch: 106. Acc: 0.132711. Loss: 2.686417. Batch_acc: 0.142525. Batch_loss: 2.447489 \n",
      "Batch: 107. Acc: 0.132905. Loss: 2.684240. Batch_acc: 0.153670. Batch_loss: 2.452241 \n",
      "Batch: 108. Acc: 0.133026. Loss: 2.682116. Batch_acc: 0.146172. Batch_loss: 2.450958 \n",
      "Batch: 109. Acc: 0.133260. Loss: 2.679764. Batch_acc: 0.159224. Batch_loss: 2.418159 \n",
      "Batch: 110. Acc: 0.133445. Loss: 2.677513. Batch_acc: 0.154162. Batch_loss: 2.425469 \n",
      "Batch: 111. Acc: 0.133831. Loss: 2.675041. Batch_acc: 0.176879. Batch_loss: 2.399680 \n",
      "Batch: 112. Acc: 0.134065. Loss: 2.672797. Batch_acc: 0.160465. Batch_loss: 2.419069 \n",
      "Batch: 113. Acc: 0.134131. Loss: 2.670616. Batch_acc: 0.141700. Batch_loss: 2.423159 \n",
      "Batch: 114. Acc: 0.134435. Loss: 2.668457. Batch_acc: 0.170137. Batch_loss: 2.414231 \n",
      "Batch: 115. Acc: 0.134811. Loss: 2.666019. Batch_acc: 0.178098. Batch_loss: 2.385639 \n",
      "Batch: 116. Acc: 0.135172. Loss: 2.663633. Batch_acc: 0.176271. Batch_loss: 2.392212 \n",
      "Batch: 117. Acc: 0.135613. Loss: 2.661332. Batch_acc: 0.187392. Batch_loss: 2.391095 \n",
      "Batch: 118. Acc: 0.135929. Loss: 2.659165. Batch_acc: 0.173188. Batch_loss: 2.403776 \n",
      "Batch: 119. Acc: 0.136349. Loss: 2.656861. Batch_acc: 0.187207. Batch_loss: 2.377528 \n",
      "Batch: 120. Acc: 0.136730. Loss: 2.654545. Batch_acc: 0.182821. Batch_loss: 2.374616 \n",
      "Batch: 121. Acc: 0.137266. Loss: 2.651984. Batch_acc: 0.200676. Batch_loss: 2.348871 \n",
      "Batch: 122. Acc: 0.137494. Loss: 2.649837. Batch_acc: 0.165980. Batch_loss: 2.382291 \n",
      "Batch: 123. Acc: 0.137875. Loss: 2.647654. Batch_acc: 0.183986. Batch_loss: 2.383065 \n",
      "Batch: 124. Acc: 0.138305. Loss: 2.645354. Batch_acc: 0.190909. Batch_loss: 2.364104 \n",
      "Batch: 125. Acc: 0.138688. Loss: 2.643261. Batch_acc: 0.187537. Batch_loss: 2.376214 \n",
      "Batch: 126. Acc: 0.139132. Loss: 2.640819. Batch_acc: 0.193207. Batch_loss: 2.343487 \n",
      "Batch: 127. Acc: 0.139431. Loss: 2.638487. Batch_acc: 0.176273. Batch_loss: 2.350834 \n",
      "Batch: 128. Acc: 0.139903. Loss: 2.636114. Batch_acc: 0.199772. Batch_loss: 2.334998 \n",
      "Batch: 129. Acc: 0.140371. Loss: 2.633806. Batch_acc: 0.199661. Batch_loss: 2.341424 \n",
      "Batch: 130. Acc: 0.140850. Loss: 2.631557. Batch_acc: 0.204720. Batch_loss: 2.331921 \n",
      "Batch: 131. Acc: 0.141278. Loss: 2.629307. Batch_acc: 0.198473. Batch_loss: 2.328820 \n",
      "Batch: 132. Acc: 0.141782. Loss: 2.626902. Batch_acc: 0.207644. Batch_loss: 2.312442 \n",
      "Batch: 133. Acc: 0.142271. Loss: 2.624532. Batch_acc: 0.206602. Batch_loss: 2.313025 \n",
      "Batch: 134. Acc: 0.142617. Loss: 2.622442. Batch_acc: 0.189504. Batch_loss: 2.338878 \n",
      "Batch: 135. Acc: 0.143027. Loss: 2.620195. Batch_acc: 0.197398. Batch_loss: 2.322241 \n",
      "Batch: 136. Acc: 0.143418. Loss: 2.618095. Batch_acc: 0.197763. Batch_loss: 2.326267 \n",
      "Batch: 137. Acc: 0.143786. Loss: 2.615948. Batch_acc: 0.193131. Batch_loss: 2.328412 \n",
      "Batch: 138. Acc: 0.144156. Loss: 2.613730. Batch_acc: 0.195690. Batch_loss: 2.304079 \n",
      "Batch: 139. Acc: 0.144491. Loss: 2.611633. Batch_acc: 0.190368. Batch_loss: 2.324875 \n",
      "Batch: 140. Acc: 0.144828. Loss: 2.609656. Batch_acc: 0.193148. Batch_loss: 2.325724 \n",
      "Batch: 141. Acc: 0.145162. Loss: 2.607622. Batch_acc: 0.192486. Batch_loss: 2.319818 \n",
      "Batch: 142. Acc: 0.145490. Loss: 2.605713. Batch_acc: 0.191671. Batch_loss: 2.337183 \n",
      "Batch: 143. Acc: 0.145943. Loss: 2.603578. Batch_acc: 0.210436. Batch_loss: 2.299616 \n",
      "Batch: 144. Acc: 0.146422. Loss: 2.601344. Batch_acc: 0.215777. Batch_loss: 2.277386 \n",
      "Batch: 145. Acc: 0.146849. Loss: 2.599236. Batch_acc: 0.208214. Batch_loss: 2.296439 \n",
      "Batch: 146. Acc: 0.147270. Loss: 2.597220. Batch_acc: 0.208621. Batch_loss: 2.303451 \n",
      "Batch: 147. Acc: 0.147630. Loss: 2.595256. Batch_acc: 0.200114. Batch_loss: 2.308672 \n",
      "Batch: 148. Acc: 0.148012. Loss: 2.593394. Batch_acc: 0.204861. Batch_loss: 2.316449 \n",
      "Batch: 149. Acc: 0.148452. Loss: 2.591354. Batch_acc: 0.210989. Batch_loss: 2.301255 \n",
      "Batch: 150. Acc: 0.148797. Loss: 2.589533. Batch_acc: 0.200926. Batch_loss: 2.314876 \n",
      "Batch: 151. Acc: 0.149178. Loss: 2.587604. Batch_acc: 0.208831. Batch_loss: 2.285735 \n",
      "Batch: 152. Acc: 0.149647. Loss: 2.585592. Batch_acc: 0.221132. Batch_loss: 2.278827 \n",
      "Batch: 153. Acc: 0.150096. Loss: 2.583528. Batch_acc: 0.218161. Batch_loss: 2.270352 \n",
      "Batch: 154. Acc: 0.150594. Loss: 2.581522. Batch_acc: 0.227273. Batch_loss: 2.272876 \n",
      "Batch: 155. Acc: 0.151052. Loss: 2.579506. Batch_acc: 0.222158. Batch_loss: 2.266403 \n",
      "Batch: 156. Acc: 0.151464. Loss: 2.577718. Batch_acc: 0.217009. Batch_loss: 2.293654 \n",
      "Batch: 157. Acc: 0.151818. Loss: 2.575840. Batch_acc: 0.205833. Batch_loss: 2.288545 \n",
      "Batch: 158. Acc: 0.152219. Loss: 2.574022. Batch_acc: 0.215393. Batch_loss: 2.287492 \n",
      "Batch: 159. Acc: 0.152624. Loss: 2.572090. Batch_acc: 0.215941. Batch_loss: 2.270583 \n",
      "Batch: 160. Acc: 0.153037. Loss: 2.570258. Batch_acc: 0.218966. Batch_loss: 2.277638 \n",
      "Batch: 161. Acc: 0.153487. Loss: 2.568378. Batch_acc: 0.224478. Batch_loss: 2.271857 \n",
      "Batch: 162. Acc: 0.153875. Loss: 2.566646. Batch_acc: 0.216840. Batch_loss: 2.285497 \n",
      "Batch: 163. Acc: 0.154236. Loss: 2.564899. Batch_acc: 0.213581. Batch_loss: 2.277840 \n",
      "Batch: 164. Acc: 0.154728. Loss: 2.563018. Batch_acc: 0.235566. Batch_loss: 2.253631 \n",
      "Batch: 165. Acc: 0.155075. Loss: 2.561291. Batch_acc: 0.212000. Batch_loss: 2.278383 \n",
      "Batch: 166. Acc: 0.155481. Loss: 2.559571. Batch_acc: 0.224514. Batch_loss: 2.267301 \n",
      "Batch: 167. Acc: 0.155845. Loss: 2.557810. Batch_acc: 0.217063. Batch_loss: 2.261289 \n",
      "Batch: 168. Acc: 0.156246. Loss: 2.556136. Batch_acc: 0.223631. Batch_loss: 2.274608 \n",
      "Batch: 169. Acc: 0.156620. Loss: 2.554252. Batch_acc: 0.219303. Batch_loss: 2.238417 \n",
      "Batch: 170. Acc: 0.157126. Loss: 2.552384. Batch_acc: 0.240648. Batch_loss: 2.244395 \n",
      "Batch: 171. Acc: 0.157489. Loss: 2.550826. Batch_acc: 0.220597. Batch_loss: 2.280081 \n",
      "Batch: 172. Acc: 0.157850. Loss: 2.549136. Batch_acc: 0.222623. Batch_loss: 2.245297 \n",
      "Batch: 173. Acc: 0.158213. Loss: 2.547637. Batch_acc: 0.220387. Batch_loss: 2.291136 \n",
      "Batch: 174. Acc: 0.158600. Loss: 2.546084. Batch_acc: 0.226273. Batch_loss: 2.274493 \n",
      "Batch: 175. Acc: 0.158985. Loss: 2.544349. Batch_acc: 0.225641. Batch_loss: 2.243965 \n",
      "Batch: 176. Acc: 0.159436. Loss: 2.542696. Batch_acc: 0.238780. Batch_loss: 2.252008 \n",
      "Batch: 177. Acc: 0.159775. Loss: 2.541095. Batch_acc: 0.219610. Batch_loss: 2.258908 \n",
      "Batch: 178. Acc: 0.160085. Loss: 2.539521. Batch_acc: 0.215066. Batch_loss: 2.259685 \n",
      "Batch: 179. Acc: 0.160510. Loss: 2.538012. Batch_acc: 0.238376. Batch_loss: 2.261756 \n",
      "Batch: 180. Acc: 0.160883. Loss: 2.536354. Batch_acc: 0.227015. Batch_loss: 2.242289 \n",
      "Batch: 181. Acc: 0.161269. Loss: 2.534785. Batch_acc: 0.228508. Batch_loss: 2.261130 \n",
      "Batch: 182. Acc: 0.161686. Loss: 2.533236. Batch_acc: 0.238623. Batch_loss: 2.247580 \n",
      "Batch: 183. Acc: 0.162085. Loss: 2.531612. Batch_acc: 0.234822. Batch_loss: 2.235895 \n",
      "Batch: 184. Acc: 0.162411. Loss: 2.530210. Batch_acc: 0.223063. Batch_loss: 2.269296 \n",
      "Batch: 185. Acc: 0.162680. Loss: 2.528806. Batch_acc: 0.213201. Batch_loss: 2.265315 \n",
      "Batch: 186. Acc: 0.163001. Loss: 2.527360. Batch_acc: 0.221969. Batch_loss: 2.261527 \n",
      "Batch: 187. Acc: 0.163447. Loss: 2.525887. Batch_acc: 0.245209. Batch_loss: 2.256162 \n",
      "Batch: 188. Acc: 0.163757. Loss: 2.524445. Batch_acc: 0.221274. Batch_loss: 2.256598 \n",
      "Batch: 189. Acc: 0.164015. Loss: 2.523021. Batch_acc: 0.212766. Batch_loss: 2.254100 \n",
      "Batch: 190. Acc: 0.164306. Loss: 2.521599. Batch_acc: 0.219597. Batch_loss: 2.251136 \n",
      "Batch: 191. Acc: 0.164673. Loss: 2.520234. Batch_acc: 0.234384. Batch_loss: 2.260558 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 192. Acc: 0.165023. Loss: 2.518801. Batch_acc: 0.233100. Batch_loss: 2.240252 \n",
      "Batch: 193. Acc: 0.165337. Loss: 2.517332. Batch_acc: 0.224341. Batch_loss: 2.241188 \n",
      "Batch: 194. Acc: 0.165625. Loss: 2.515889. Batch_acc: 0.220957. Batch_loss: 2.238852 \n",
      "Batch: 195. Acc: 0.165971. Loss: 2.514502. Batch_acc: 0.234880. Batch_loss: 2.238497 \n",
      "Batch: 196. Acc: 0.166341. Loss: 2.513063. Batch_acc: 0.239420. Batch_loss: 2.229047 \n",
      "Batch: 197. Acc: 0.166680. Loss: 2.511684. Batch_acc: 0.233661. Batch_loss: 2.238823 \n",
      "Batch: 198. Acc: 0.166969. Loss: 2.510322. Batch_acc: 0.224267. Batch_loss: 2.240872 \n",
      "Batch: 199. Acc: 0.167245. Loss: 2.509036. Batch_acc: 0.223335. Batch_loss: 2.246944 \n",
      "Batch: 200. Acc: 0.167565. Loss: 2.507583. Batch_acc: 0.231926. Batch_loss: 2.215740 \n",
      "Batch: 201. Acc: 0.167877. Loss: 2.506270. Batch_acc: 0.230903. Batch_loss: 2.240991 \n",
      "Batch: 202. Acc: 0.168119. Loss: 2.505044. Batch_acc: 0.217926. Batch_loss: 2.253050 \n",
      "Batch: 203. Acc: 0.168430. Loss: 2.503738. Batch_acc: 0.231567. Batch_loss: 2.238489 \n",
      "Batch: 204. Acc: 0.168761. Loss: 2.502299. Batch_acc: 0.235127. Batch_loss: 2.213310 \n",
      "Batch: 205. Acc: 0.169086. Loss: 2.500957. Batch_acc: 0.234234. Batch_loss: 2.232049 \n",
      "Batch: 206. Acc: 0.169334. Loss: 2.499688. Batch_acc: 0.219581. Batch_loss: 2.242627 \n",
      "Batch: 207. Acc: 0.169532. Loss: 2.498494. Batch_acc: 0.210017. Batch_loss: 2.254074 \n",
      "Batch: 208. Acc: 0.169833. Loss: 2.497172. Batch_acc: 0.231029. Batch_loss: 2.228633 \n",
      "Batch: 209. Acc: 0.170131. Loss: 2.495939. Batch_acc: 0.233236. Batch_loss: 2.234879 \n",
      "Batch: 210. Acc: 0.170422. Loss: 2.494718. Batch_acc: 0.233412. Batch_loss: 2.230847 \n",
      "Batch: 211. Acc: 0.170666. Loss: 2.493582. Batch_acc: 0.223204. Batch_loss: 2.248205 \n",
      "Batch: 212. Acc: 0.171061. Loss: 2.492201. Batch_acc: 0.254744. Batch_loss: 2.199793 \n",
      "Batch: 213. Acc: 0.171263. Loss: 2.491080. Batch_acc: 0.214785. Batch_loss: 2.249689 \n",
      "Batch: 214. Acc: 0.171559. Loss: 2.489821. Batch_acc: 0.235706. Batch_loss: 2.216738 \n",
      "Batch: 215. Acc: 0.171835. Loss: 2.488565. Batch_acc: 0.232070. Batch_loss: 2.215079 \n",
      "Batch: 216. Acc: 0.172098. Loss: 2.487450. Batch_acc: 0.229912. Batch_loss: 2.242306 \n",
      "Batch: 217. Acc: 0.172292. Loss: 2.486422. Batch_acc: 0.214617. Batch_loss: 2.261650 \n",
      "Batch: 218. Acc: 0.172533. Loss: 2.485239. Batch_acc: 0.225173. Batch_loss: 2.226649 \n",
      "Batch: 219. Acc: 0.172860. Loss: 2.483924. Batch_acc: 0.242577. Batch_loss: 2.203692 \n",
      "Batch: 220. Acc: 0.173099. Loss: 2.482759. Batch_acc: 0.225676. Batch_loss: 2.226545 \n",
      "Batch: 221. Acc: 0.173287. Loss: 2.481762. Batch_acc: 0.217338. Batch_loss: 2.248164 \n",
      "Batch: 222. Acc: 0.173579. Loss: 2.480566. Batch_acc: 0.237635. Batch_loss: 2.218545 \n",
      "Batch: 223. Acc: 0.173814. Loss: 2.479402. Batch_acc: 0.225992. Batch_loss: 2.220145 \n",
      "Batch: 224. Acc: 0.174145. Loss: 2.478156. Batch_acc: 0.246094. Batch_loss: 2.207779 \n",
      "Batch: 225. Acc: 0.174446. Loss: 2.476984. Batch_acc: 0.243402. Batch_loss: 2.208242 \n",
      "Batch: 226. Acc: 0.174685. Loss: 2.475838. Batch_acc: 0.228473. Batch_loss: 2.217692 \n",
      "Batch: 227. Acc: 0.174917. Loss: 2.474693. Batch_acc: 0.228305. Batch_loss: 2.211879 \n",
      "Batch: 228. Acc: 0.175132. Loss: 2.473684. Batch_acc: 0.224680. Batch_loss: 2.241241 \n",
      "Batch: 229. Acc: 0.175317. Loss: 2.472635. Batch_acc: 0.218023. Batch_loss: 2.230140 \n",
      "Batch: 230. Acc: 0.175619. Loss: 2.471534. Batch_acc: 0.244432. Batch_loss: 2.220249 \n",
      "Batch: 231. Acc: 0.175817. Loss: 2.470507. Batch_acc: 0.222157. Batch_loss: 2.230505 \n",
      "Batch: 232. Acc: 0.176095. Loss: 2.469401. Batch_acc: 0.242192. Batch_loss: 2.206720 \n",
      "Batch: 233. Acc: 0.176345. Loss: 2.468245. Batch_acc: 0.233465. Batch_loss: 2.203878 \n",
      "Batch: 234. Acc: 0.176573. Loss: 2.467242. Batch_acc: 0.229238. Batch_loss: 2.235448 \n",
      "Batch: 235. Acc: 0.176795. Loss: 2.466113. Batch_acc: 0.228441. Batch_loss: 2.203137 \n",
      "Batch: 236. Acc: 0.177082. Loss: 2.465038. Batch_acc: 0.244432. Batch_loss: 2.213343 \n",
      "Batch: 237. Acc: 0.177343. Loss: 2.464048. Batch_acc: 0.240047. Batch_loss: 2.225526 \n",
      "Batch: 238. Acc: 0.177608. Loss: 2.463001. Batch_acc: 0.239660. Batch_loss: 2.217813 \n",
      "Batch: 239. Acc: 0.177864. Loss: 2.461956. Batch_acc: 0.238695. Batch_loss: 2.213726 \n",
      "Batch: 240. Acc: 0.178127. Loss: 2.460921. Batch_acc: 0.239022. Batch_loss: 2.221112 \n",
      "Batch: 241. Acc: 0.178381. Loss: 2.459914. Batch_acc: 0.241420. Batch_loss: 2.210551 \n",
      "Batch: 242. Acc: 0.178649. Loss: 2.458805. Batch_acc: 0.243259. Batch_loss: 2.191521 \n",
      "Batch: 243. Acc: 0.178845. Loss: 2.457867. Batch_acc: 0.226027. Batch_loss: 2.231937 \n",
      "Batch: 244. Acc: 0.179081. Loss: 2.456827. Batch_acc: 0.234802. Batch_loss: 2.210980 \n",
      "Batch: 245. Acc: 0.179289. Loss: 2.455829. Batch_acc: 0.230636. Batch_loss: 2.210471 \n",
      "Batch: 246. Acc: 0.179557. Loss: 2.454826. Batch_acc: 0.245940. Batch_loss: 2.206165 \n",
      "Batch: 247. Acc: 0.179801. Loss: 2.453832. Batch_acc: 0.240936. Batch_loss: 2.204519 \n",
      "Batch: 248. Acc: 0.180047. Loss: 2.452691. Batch_acc: 0.239374. Batch_loss: 2.177854 \n",
      "Batch: 249. Acc: 0.180279. Loss: 2.451702. Batch_acc: 0.237443. Batch_loss: 2.207464 \n",
      "Batch: 250. Acc: 0.180575. Loss: 2.450578. Batch_acc: 0.252667. Batch_loss: 2.176547 \n",
      "Batch: 251. Acc: 0.180844. Loss: 2.449538. Batch_acc: 0.249561. Batch_loss: 2.183904 \n",
      "Checkpointing on batch: 251. Accuracy: 0.18084352983103816. Loss per char: 2.449538187749582. Time: 1627202816.218363\n",
      "Last question is tensor([ 2, 36, 66, 77, 68, 86, 77, 66, 85, 70,  1, 14, 18, 15, 20,  1, 12,  1,\n",
      "        19, 17, 23, 17, 22, 15, 25, 25, 18, 24, 15,  3,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0], device='cuda:0')\n",
      "Removing existing model file at checkpoints\\add_or_sub-2021-07-25_latest_checkpoint.pth\n",
      "No existing model file found\n",
      "Starting checkpoint save of checkpoints\\add_or_sub-2021-07-25_latest_checkpoint.pth...\n",
      "Final saved model size: 530790396\n",
      "Batch: 252. Acc: 0.181053. Loss: 2.448614. Batch_acc: 0.234203. Batch_loss: 2.214108 \n",
      "Batch: 253. Acc: 0.181312. Loss: 2.447602. Batch_acc: 0.245219. Batch_loss: 2.197336 \n",
      "Batch: 254. Acc: 0.181479. Loss: 2.446731. Batch_acc: 0.225327. Batch_loss: 2.218452 \n",
      "Batch: 255. Acc: 0.181718. Loss: 2.445785. Batch_acc: 0.243448. Batch_loss: 2.201633 \n",
      "Batch: 256. Acc: 0.181883. Loss: 2.444936. Batch_acc: 0.224078. Batch_loss: 2.227595 \n",
      "Batch: 257. Acc: 0.182131. Loss: 2.443761. Batch_acc: 0.242756. Batch_loss: 2.156818 \n",
      "Batch: 258. Acc: 0.182340. Loss: 2.442928. Batch_acc: 0.236103. Batch_loss: 2.229074 \n",
      "Batch: 259. Acc: 0.182537. Loss: 2.442075. Batch_acc: 0.233661. Batch_loss: 2.220027 \n",
      "Batch: 260. Acc: 0.182789. Loss: 2.441087. Batch_acc: 0.249127. Batch_loss: 2.181545 \n",
      "Batch: 261. Acc: 0.183072. Loss: 2.440093. Batch_acc: 0.257077. Batch_loss: 2.179527 \n",
      "Batch: 262. Acc: 0.183256. Loss: 2.439335. Batch_acc: 0.232300. Batch_loss: 2.237582 \n",
      "Batch: 263. Acc: 0.183400. Loss: 2.438479. Batch_acc: 0.220705. Batch_loss: 2.215917 \n",
      "Batch: 264. Acc: 0.183607. Loss: 2.437656. Batch_acc: 0.238728. Batch_loss: 2.219508 \n",
      "Batch: 265. Acc: 0.183803. Loss: 2.436847. Batch_acc: 0.236780. Batch_loss: 2.218270 \n",
      "Batch: 266. Acc: 0.183969. Loss: 2.436089. Batch_acc: 0.228571. Batch_loss: 2.231712 \n",
      "Batch: 267. Acc: 0.184149. Loss: 2.435287. Batch_acc: 0.233061. Batch_loss: 2.218111 \n",
      "Batch: 268. Acc: 0.184346. Loss: 2.434481. Batch_acc: 0.237984. Batch_loss: 2.214690 \n",
      "Batch: 269. Acc: 0.184537. Loss: 2.433575. Batch_acc: 0.236353. Batch_loss: 2.187736 \n",
      "Batch: 270. Acc: 0.184672. Loss: 2.432768. Batch_acc: 0.221198. Batch_loss: 2.214612 \n",
      "Batch: 271. Acc: 0.184855. Loss: 2.431974. Batch_acc: 0.235467. Batch_loss: 2.212598 \n",
      "Batch: 272. Acc: 0.185014. Loss: 2.431134. Batch_acc: 0.228923. Batch_loss: 2.199027 \n",
      "Batch: 273. Acc: 0.185194. Loss: 2.430371. Batch_acc: 0.235053. Batch_loss: 2.218376 \n",
      "Batch: 274. Acc: 0.185441. Loss: 2.429468. Batch_acc: 0.253012. Batch_loss: 2.182936 \n",
      "Batch: 275. Acc: 0.185626. Loss: 2.428680. Batch_acc: 0.236479. Batch_loss: 2.212238 \n",
      "Batch: 276. Acc: 0.185776. Loss: 2.427928. Batch_acc: 0.227089. Batch_loss: 2.220224 \n",
      "Batch: 277. Acc: 0.185931. Loss: 2.427183. Batch_acc: 0.228702. Batch_loss: 2.222127 \n",
      "Batch: 278. Acc: 0.186086. Loss: 2.426465. Batch_acc: 0.230541. Batch_loss: 2.220641 \n",
      "Batch: 279. Acc: 0.186290. Loss: 2.425669. Batch_acc: 0.243088. Batch_loss: 2.203577 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 280. Acc: 0.186494. Loss: 2.424831. Batch_acc: 0.243151. Batch_loss: 2.192437 \n",
      "Batch: 281. Acc: 0.186665. Loss: 2.424046. Batch_acc: 0.234152. Batch_loss: 2.205135 \n",
      "Batch: 282. Acc: 0.186832. Loss: 2.423288. Batch_acc: 0.233352. Batch_loss: 2.212170 \n",
      "Batch: 283. Acc: 0.187016. Loss: 2.422621. Batch_acc: 0.239080. Batch_loss: 2.234367 \n",
      "Batch: 284. Acc: 0.187215. Loss: 2.421842. Batch_acc: 0.243028. Batch_loss: 2.202984 \n",
      "Batch: 285. Acc: 0.187401. Loss: 2.421088. Batch_acc: 0.241481. Batch_loss: 2.202091 \n",
      "Batch: 286. Acc: 0.187581. Loss: 2.420349. Batch_acc: 0.239650. Batch_loss: 2.206257 \n",
      "Batch: 287. Acc: 0.187758. Loss: 2.419659. Batch_acc: 0.238754. Batch_loss: 2.221364 \n",
      "Batch: 288. Acc: 0.187961. Loss: 2.418887. Batch_acc: 0.245594. Batch_loss: 2.199306 \n",
      "Batch: 289. Acc: 0.188189. Loss: 2.418107. Batch_acc: 0.253273. Batch_loss: 2.195477 \n",
      "Batch: 290. Acc: 0.188399. Loss: 2.417289. Batch_acc: 0.247486. Batch_loss: 2.187320 \n",
      "Batch: 291. Acc: 0.188566. Loss: 2.416506. Batch_acc: 0.237054. Batch_loss: 2.188647 \n",
      "Batch: 292. Acc: 0.188760. Loss: 2.415726. Batch_acc: 0.245132. Batch_loss: 2.189266 \n",
      "Batch: 293. Acc: 0.188929. Loss: 2.415044. Batch_acc: 0.239181. Batch_loss: 2.212038 \n",
      "Batch: 294. Acc: 0.189080. Loss: 2.414295. Batch_acc: 0.233087. Batch_loss: 2.196914 \n",
      "Batch: 295. Acc: 0.189259. Loss: 2.413614. Batch_acc: 0.242215. Batch_loss: 2.212415 \n",
      "Batch: 296. Acc: 0.189443. Loss: 2.412809. Batch_acc: 0.242906. Batch_loss: 2.178144 \n",
      "Batch: 297. Acc: 0.189579. Loss: 2.412077. Batch_acc: 0.229296. Batch_loss: 2.199213 \n",
      "Batch: 298. Acc: 0.189795. Loss: 2.411253. Batch_acc: 0.253731. Batch_loss: 2.166441 \n",
      "Batch: 299. Acc: 0.189945. Loss: 2.410569. Batch_acc: 0.233613. Batch_loss: 2.211548 \n",
      "Batch: 300. Acc: 0.190147. Loss: 2.409792. Batch_acc: 0.250572. Batch_loss: 2.178384 \n",
      "Batch: 301. Acc: 0.190311. Loss: 2.409081. Batch_acc: 0.240351. Batch_loss: 2.191542 \n",
      "Batch: 302. Acc: 0.190493. Loss: 2.408362. Batch_acc: 0.245392. Batch_loss: 2.191306 \n",
      "Batch: 303. Acc: 0.190611. Loss: 2.407700. Batch_acc: 0.226307. Batch_loss: 2.207418 \n",
      "Batch: 304. Acc: 0.190737. Loss: 2.407056. Batch_acc: 0.229604. Batch_loss: 2.208856 \n",
      "Batch: 305. Acc: 0.190922. Loss: 2.406355. Batch_acc: 0.245920. Batch_loss: 2.197397 \n",
      "Batch: 306. Acc: 0.191047. Loss: 2.405720. Batch_acc: 0.230723. Batch_loss: 2.204145 \n",
      "Batch: 307. Acc: 0.191170. Loss: 2.405103. Batch_acc: 0.228620. Batch_loss: 2.217475 \n",
      "Batch: 308. Acc: 0.191351. Loss: 2.404340. Batch_acc: 0.246716. Batch_loss: 2.171185 \n",
      "Batch: 309. Acc: 0.191496. Loss: 2.403645. Batch_acc: 0.235795. Batch_loss: 2.191863 \n",
      "Batch: 310. Acc: 0.191658. Loss: 2.402914. Batch_acc: 0.242442. Batch_loss: 2.173796 \n",
      "Batch: 311. Acc: 0.191846. Loss: 2.402118. Batch_acc: 0.247920. Batch_loss: 2.163775 \n",
      "Batch: 312. Acc: 0.192027. Loss: 2.401349. Batch_acc: 0.247738. Batch_loss: 2.165552 \n",
      "Batch: 313. Acc: 0.192214. Loss: 2.400582. Batch_acc: 0.249717. Batch_loss: 2.164615 \n",
      "Batch: 314. Acc: 0.192347. Loss: 2.399932. Batch_acc: 0.233848. Batch_loss: 2.196970 \n",
      "Batch: 315. Acc: 0.192517. Loss: 2.399232. Batch_acc: 0.245304. Batch_loss: 2.181424 \n",
      "Batch: 316. Acc: 0.192652. Loss: 2.398535. Batch_acc: 0.236119. Batch_loss: 2.174753 \n",
      "Batch: 317. Acc: 0.192788. Loss: 2.397915. Batch_acc: 0.236427. Batch_loss: 2.198766 \n",
      "Batch: 318. Acc: 0.192911. Loss: 2.397302. Batch_acc: 0.232382. Batch_loss: 2.199997 \n",
      "Batch: 319. Acc: 0.193069. Loss: 2.396618. Batch_acc: 0.243538. Batch_loss: 2.178784 \n",
      "Batch: 320. Acc: 0.193293. Loss: 2.395861. Batch_acc: 0.263755. Batch_loss: 2.157097 \n",
      "Batch: 321. Acc: 0.193496. Loss: 2.395163. Batch_acc: 0.259453. Batch_loss: 2.168837 \n",
      "Batch: 322. Acc: 0.193605. Loss: 2.394586. Batch_acc: 0.228150. Batch_loss: 2.211347 \n",
      "Batch: 323. Acc: 0.193747. Loss: 2.393911. Batch_acc: 0.239679. Batch_loss: 2.176944 \n",
      "Batch: 324. Acc: 0.193895. Loss: 2.393277. Batch_acc: 0.240449. Batch_loss: 2.192698 \n",
      "Batch: 325. Acc: 0.194030. Loss: 2.392750. Batch_acc: 0.238040. Batch_loss: 2.221263 \n",
      "Batch: 326. Acc: 0.194140. Loss: 2.392172. Batch_acc: 0.229630. Batch_loss: 2.205455 \n",
      "Batch: 327. Acc: 0.194293. Loss: 2.391638. Batch_acc: 0.245173. Batch_loss: 2.214170 \n",
      "Batch: 328. Acc: 0.194395. Loss: 2.391064. Batch_acc: 0.227740. Batch_loss: 2.204439 \n",
      "Batch: 329. Acc: 0.194590. Loss: 2.390411. Batch_acc: 0.258138. Batch_loss: 2.177092 \n",
      "Batch: 330. Acc: 0.194702. Loss: 2.389863. Batch_acc: 0.232682. Batch_loss: 2.203839 \n",
      "Batch: 331. Acc: 0.194850. Loss: 2.389282. Batch_acc: 0.244718. Batch_loss: 2.193309 \n",
      "Batch: 332. Acc: 0.194974. Loss: 2.388654. Batch_acc: 0.236981. Batch_loss: 2.176677 \n",
      "Batch: 333. Acc: 0.195114. Loss: 2.388119. Batch_acc: 0.242001. Batch_loss: 2.208018 \n",
      "Batch: 334. Acc: 0.195259. Loss: 2.387534. Batch_acc: 0.243568. Batch_loss: 2.193481 \n",
      "Batch: 335. Acc: 0.195435. Loss: 2.386864. Batch_acc: 0.252649. Batch_loss: 2.169522 \n",
      "Batch: 336. Acc: 0.195574. Loss: 2.386280. Batch_acc: 0.241458. Batch_loss: 2.192215 \n",
      "Batch: 337. Acc: 0.195768. Loss: 2.385570. Batch_acc: 0.261194. Batch_loss: 2.146805 \n",
      "Batch: 338. Acc: 0.195917. Loss: 2.384950. Batch_acc: 0.247066. Batch_loss: 2.171329 \n",
      "Batch: 339. Acc: 0.196086. Loss: 2.384367. Batch_acc: 0.253594. Batch_loss: 2.186962 \n",
      "Batch: 340. Acc: 0.196233. Loss: 2.383772. Batch_acc: 0.245845. Batch_loss: 2.182478 \n",
      "Batch: 341. Acc: 0.196354. Loss: 2.383215. Batch_acc: 0.237929. Batch_loss: 2.191090 \n",
      "Batch: 342. Acc: 0.196518. Loss: 2.382673. Batch_acc: 0.252728. Batch_loss: 2.197911 \n",
      "Batch: 343. Acc: 0.196581. Loss: 2.382305. Batch_acc: 0.218330. Batch_loss: 2.254170 \n",
      "Batch: 344. Acc: 0.196706. Loss: 2.381766. Batch_acc: 0.238227. Batch_loss: 2.203321 \n",
      "Batch: 345. Acc: 0.196871. Loss: 2.381156. Batch_acc: 0.252394. Batch_loss: 2.175262 \n",
      "Batch: 346. Acc: 0.197026. Loss: 2.380609. Batch_acc: 0.250142. Batch_loss: 2.193913 \n",
      "Batch: 347. Acc: 0.197114. Loss: 2.380169. Batch_acc: 0.228070. Batch_loss: 2.225239 \n",
      "Batch: 348. Acc: 0.197239. Loss: 2.379599. Batch_acc: 0.241869. Batch_loss: 2.175565 \n",
      "Batch: 349. Acc: 0.197342. Loss: 2.379089. Batch_acc: 0.233547. Batch_loss: 2.199131 \n",
      "Batch: 350. Acc: 0.197461. Loss: 2.378529. Batch_acc: 0.238527. Batch_loss: 2.185675 \n",
      "Batch: 351. Acc: 0.197572. Loss: 2.377998. Batch_acc: 0.237119. Batch_loss: 2.188417 \n",
      "Batch: 352. Acc: 0.197764. Loss: 2.377370. Batch_acc: 0.266162. Batch_loss: 2.153741 \n",
      "Batch: 353. Acc: 0.197945. Loss: 2.376724. Batch_acc: 0.262850. Batch_loss: 2.145116 \n",
      "Batch: 354. Acc: 0.198052. Loss: 2.376211. Batch_acc: 0.236780. Batch_loss: 2.190943 \n",
      "Batch: 355. Acc: 0.198203. Loss: 2.375637. Batch_acc: 0.251576. Batch_loss: 2.172899 \n",
      "Batch: 356. Acc: 0.198340. Loss: 2.375071. Batch_acc: 0.246724. Batch_loss: 2.175620 \n",
      "Batch: 357. Acc: 0.198464. Loss: 2.374534. Batch_acc: 0.242424. Batch_loss: 2.184164 \n",
      "Batch: 358. Acc: 0.198553. Loss: 2.374101. Batch_acc: 0.230501. Batch_loss: 2.217266 \n",
      "Batch: 359. Acc: 0.198659. Loss: 2.373575. Batch_acc: 0.236395. Batch_loss: 2.187551 \n",
      "Batch: 360. Acc: 0.198819. Loss: 2.372924. Batch_acc: 0.256827. Batch_loss: 2.136278 \n",
      "Batch: 361. Acc: 0.198929. Loss: 2.372422. Batch_acc: 0.239412. Batch_loss: 2.187512 \n",
      "Batch: 362. Acc: 0.199025. Loss: 2.371892. Batch_acc: 0.233015. Batch_loss: 2.184605 \n",
      "Batch: 363. Acc: 0.199152. Loss: 2.371364. Batch_acc: 0.245911. Batch_loss: 2.176933 \n",
      "Batch: 364. Acc: 0.199261. Loss: 2.370849. Batch_acc: 0.238122. Batch_loss: 2.186714 \n",
      "Batch: 365. Acc: 0.199398. Loss: 2.370297. Batch_acc: 0.250000. Batch_loss: 2.167183 \n",
      "Batch: 366. Acc: 0.199477. Loss: 2.369880. Batch_acc: 0.227941. Batch_loss: 2.219907 \n",
      "Batch: 367. Acc: 0.199663. Loss: 2.369272. Batch_acc: 0.266106. Batch_loss: 2.152318 \n",
      "Batch: 368. Acc: 0.199783. Loss: 2.368732. Batch_acc: 0.243458. Batch_loss: 2.172106 \n",
      "Batch: 369. Acc: 0.199890. Loss: 2.368188. Batch_acc: 0.238336. Batch_loss: 2.172152 \n",
      "Batch: 370. Acc: 0.199984. Loss: 2.367687. Batch_acc: 0.234554. Batch_loss: 2.183465 \n",
      "Batch: 371. Acc: 0.200115. Loss: 2.367193. Batch_acc: 0.247743. Batch_loss: 2.187525 \n",
      "Batch: 372. Acc: 0.200237. Loss: 2.366662. Batch_acc: 0.245413. Batch_loss: 2.169862 \n",
      "Batch: 373. Acc: 0.200352. Loss: 2.366180. Batch_acc: 0.244145. Batch_loss: 2.183228 \n",
      "Batch: 374. Acc: 0.200453. Loss: 2.365716. Batch_acc: 0.238537. Batch_loss: 2.190806 \n",
      "Batch: 375. Acc: 0.200560. Loss: 2.365221. Batch_acc: 0.239932. Batch_loss: 2.182346 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 376. Acc: 0.200684. Loss: 2.364721. Batch_acc: 0.247405. Batch_loss: 2.176139 \n",
      "Batch: 377. Acc: 0.200823. Loss: 2.364215. Batch_acc: 0.253303. Batch_loss: 2.174044 \n",
      "Batch: 378. Acc: 0.200938. Loss: 2.363721. Batch_acc: 0.243721. Batch_loss: 2.178491 \n",
      "Batch: 379. Acc: 0.201015. Loss: 2.363249. Batch_acc: 0.230199. Batch_loss: 2.186127 \n",
      "Batch: 380. Acc: 0.201114. Loss: 2.362784. Batch_acc: 0.238979. Batch_loss: 2.184648 \n",
      "Batch: 381. Acc: 0.201219. Loss: 2.362330. Batch_acc: 0.240847. Batch_loss: 2.190408 \n",
      "Batch: 382. Acc: 0.201287. Loss: 2.361905. Batch_acc: 0.227804. Batch_loss: 2.196945 \n",
      "Batch: 383. Acc: 0.201424. Loss: 2.361430. Batch_acc: 0.253586. Batch_loss: 2.180168 \n",
      "Batch: 384. Acc: 0.201530. Loss: 2.360897. Batch_acc: 0.242147. Batch_loss: 2.157825 \n",
      "Batch: 385. Acc: 0.201651. Loss: 2.360455. Batch_acc: 0.247991. Batch_loss: 2.190718 \n",
      "Batch: 386. Acc: 0.201767. Loss: 2.359987. Batch_acc: 0.246136. Batch_loss: 2.180077 \n",
      "Batch: 387. Acc: 0.201871. Loss: 2.359529. Batch_acc: 0.242111. Batch_loss: 2.182926 \n",
      "Batch: 388. Acc: 0.202016. Loss: 2.359064. Batch_acc: 0.258324. Batch_loss: 2.179032 \n",
      "Batch: 389. Acc: 0.202136. Loss: 2.358592. Batch_acc: 0.248565. Batch_loss: 2.175769 \n",
      "Batch: 390. Acc: 0.202285. Loss: 2.358065. Batch_acc: 0.259322. Batch_loss: 2.155961 \n",
      "Batch: 391. Acc: 0.202382. Loss: 2.357609. Batch_acc: 0.240838. Batch_loss: 2.177548 \n",
      "Batch: 392. Acc: 0.202497. Loss: 2.357097. Batch_acc: 0.247133. Batch_loss: 2.156922 \n",
      "Batch: 393. Acc: 0.202597. Loss: 2.356563. Batch_acc: 0.241991. Batch_loss: 2.148072 \n",
      "Batch: 394. Acc: 0.202699. Loss: 2.356135. Batch_acc: 0.242651. Batch_loss: 2.187154 \n",
      "Batch: 395. Acc: 0.202836. Loss: 2.355574. Batch_acc: 0.255593. Batch_loss: 2.140143 \n",
      "Batch: 396. Acc: 0.202963. Loss: 2.355103. Batch_acc: 0.253150. Batch_loss: 2.169458 \n",
      "Batch: 397. Acc: 0.203079. Loss: 2.354607. Batch_acc: 0.248996. Batch_loss: 2.158540 \n",
      "Batch: 398. Acc: 0.203190. Loss: 2.354153. Batch_acc: 0.247256. Batch_loss: 2.172667 \n",
      "Batch: 399. Acc: 0.203325. Loss: 2.353711. Batch_acc: 0.258159. Batch_loss: 2.175092 \n",
      "Batch: 400. Acc: 0.203427. Loss: 2.353247. Batch_acc: 0.244886. Batch_loss: 2.164839 \n",
      "Batch: 401. Acc: 0.203526. Loss: 2.352847. Batch_acc: 0.242702. Batch_loss: 2.193277 \n",
      "Batch: 402. Acc: 0.203601. Loss: 2.352441. Batch_acc: 0.233871. Batch_loss: 2.188789 \n",
      "Batch: 403. Acc: 0.203714. Loss: 2.352076. Batch_acc: 0.249711. Batch_loss: 2.204470 \n",
      "Batch: 404. Acc: 0.203873. Loss: 2.351541. Batch_acc: 0.267468. Batch_loss: 2.136205 \n",
      "Batch: 405. Acc: 0.203967. Loss: 2.351114. Batch_acc: 0.242690. Batch_loss: 2.175689 \n",
      "Batch: 406. Acc: 0.204061. Loss: 2.350678. Batch_acc: 0.242165. Batch_loss: 2.175126 \n",
      "Batch: 407. Acc: 0.204171. Loss: 2.350255. Batch_acc: 0.247348. Batch_loss: 2.183292 \n",
      "Batch: 408. Acc: 0.204251. Loss: 2.349879. Batch_acc: 0.238466. Batch_loss: 2.190011 \n",
      "Batch: 409. Acc: 0.204367. Loss: 2.349406. Batch_acc: 0.250847. Batch_loss: 2.159772 \n",
      "Batch: 410. Acc: 0.204468. Loss: 2.348900. Batch_acc: 0.245826. Batch_loss: 2.141094 \n",
      "Batch: 411. Acc: 0.204561. Loss: 2.348505. Batch_acc: 0.243180. Batch_loss: 2.184841 \n",
      "Batch: 412. Acc: 0.204655. Loss: 2.348086. Batch_acc: 0.242390. Batch_loss: 2.179223 \n",
      "Batch: 413. Acc: 0.204741. Loss: 2.347700. Batch_acc: 0.240390. Batch_loss: 2.188733 \n",
      "Batch: 414. Acc: 0.204837. Loss: 2.347325. Batch_acc: 0.245687. Batch_loss: 2.186826 \n",
      "Batch: 415. Acc: 0.204944. Loss: 2.346868. Batch_acc: 0.249711. Batch_loss: 2.156364 \n",
      "Batch: 416. Acc: 0.205032. Loss: 2.346455. Batch_acc: 0.241518. Batch_loss: 2.174671 \n",
      "Batch: 417. Acc: 0.205148. Loss: 2.346031. Batch_acc: 0.253295. Batch_loss: 2.169691 \n",
      "Batch: 418. Acc: 0.205238. Loss: 2.345638. Batch_acc: 0.244104. Batch_loss: 2.177586 \n",
      "Batch: 419. Acc: 0.205367. Loss: 2.345185. Batch_acc: 0.259688. Batch_loss: 2.154505 \n",
      "Batch: 420. Acc: 0.205496. Loss: 2.344751. Batch_acc: 0.259710. Batch_loss: 2.160816 \n",
      "Batch: 421. Acc: 0.205591. Loss: 2.344369. Batch_acc: 0.246616. Batch_loss: 2.180011 \n",
      "Batch: 422. Acc: 0.205697. Loss: 2.343964. Batch_acc: 0.251019. Batch_loss: 2.171105 \n",
      "Batch: 423. Acc: 0.205826. Loss: 2.343510. Batch_acc: 0.259009. Batch_loss: 2.155385 \n",
      "Batch: 424. Acc: 0.205915. Loss: 2.343071. Batch_acc: 0.242849. Batch_loss: 2.162029 \n",
      "Batch: 425. Acc: 0.206054. Loss: 2.342575. Batch_acc: 0.265815. Batch_loss: 2.129990 \n",
      "Batch: 426. Acc: 0.206136. Loss: 2.342160. Batch_acc: 0.240922. Batch_loss: 2.165062 \n",
      "Batch: 427. Acc: 0.206223. Loss: 2.341728. Batch_acc: 0.243259. Batch_loss: 2.157901 \n",
      "Batch: 428. Acc: 0.206303. Loss: 2.341382. Batch_acc: 0.240456. Batch_loss: 2.194682 \n",
      "Batch: 429. Acc: 0.206372. Loss: 2.340990. Batch_acc: 0.235361. Batch_loss: 2.174824 \n",
      "Batch: 430. Acc: 0.206442. Loss: 2.340642. Batch_acc: 0.236541. Batch_loss: 2.191842 \n",
      "Batch: 431. Acc: 0.206559. Loss: 2.340156. Batch_acc: 0.255631. Batch_loss: 2.135116 \n",
      "Batch: 432. Acc: 0.206663. Loss: 2.339737. Batch_acc: 0.252478. Batch_loss: 2.156365 \n",
      "Batch: 433. Acc: 0.206754. Loss: 2.339416. Batch_acc: 0.247337. Batch_loss: 2.196093 \n",
      "Batch: 434. Acc: 0.206834. Loss: 2.338990. Batch_acc: 0.240793. Batch_loss: 2.157313 \n",
      "Batch: 435. Acc: 0.206908. Loss: 2.338665. Batch_acc: 0.239469. Batch_loss: 2.196764 \n",
      "Batch: 436. Acc: 0.206998. Loss: 2.338252. Batch_acc: 0.245845. Batch_loss: 2.159100 \n",
      "Batch: 437. Acc: 0.207127. Loss: 2.337814. Batch_acc: 0.262415. Batch_loss: 2.149806 \n",
      "Batch: 438. Acc: 0.207241. Loss: 2.337376. Batch_acc: 0.258235. Batch_loss: 2.141563 \n",
      "Batch: 439. Acc: 0.207329. Loss: 2.337015. Batch_acc: 0.246906. Batch_loss: 2.174393 \n",
      "Batch: 440. Acc: 0.207437. Loss: 2.336590. Batch_acc: 0.255196. Batch_loss: 2.149263 \n",
      "Batch: 441. Acc: 0.207549. Loss: 2.336153. Batch_acc: 0.256734. Batch_loss: 2.144005 \n",
      "Batch: 442. Acc: 0.207647. Loss: 2.335730. Batch_acc: 0.251462. Batch_loss: 2.145792 \n",
      "Batch: 443. Acc: 0.207754. Loss: 2.335323. Batch_acc: 0.254066. Batch_loss: 2.159872 \n",
      "Batch: 444. Acc: 0.207862. Loss: 2.334901. Batch_acc: 0.255894. Batch_loss: 2.147498 \n",
      "Batch: 445. Acc: 0.207976. Loss: 2.334483. Batch_acc: 0.258960. Batch_loss: 2.147465 \n",
      "Batch: 446. Acc: 0.208075. Loss: 2.334049. Batch_acc: 0.251569. Batch_loss: 2.142534 \n",
      "Batch: 447. Acc: 0.208212. Loss: 2.333588. Batch_acc: 0.269540. Batch_loss: 2.127574 \n",
      "Batch: 448. Acc: 0.208315. Loss: 2.333184. Batch_acc: 0.254619. Batch_loss: 2.151833 \n",
      "Batch: 449. Acc: 0.208410. Loss: 2.332828. Batch_acc: 0.251594. Batch_loss: 2.171524 \n",
      "Batch: 450. Acc: 0.208523. Loss: 2.332400. Batch_acc: 0.258338. Batch_loss: 2.143302 \n",
      "Batch: 451. Acc: 0.208621. Loss: 2.332024. Batch_acc: 0.252887. Batch_loss: 2.161995 \n",
      "Batch: 452. Acc: 0.208696. Loss: 2.331635. Batch_acc: 0.242442. Batch_loss: 2.157435 \n",
      "Batch: 453. Acc: 0.208803. Loss: 2.331233. Batch_acc: 0.256077. Batch_loss: 2.152366 \n",
      "Batch: 454. Acc: 0.208876. Loss: 2.330873. Batch_acc: 0.242941. Batch_loss: 2.163398 \n",
      "Batch: 455. Acc: 0.208960. Loss: 2.330516. Batch_acc: 0.247262. Batch_loss: 2.168059 \n",
      "Batch: 456. Acc: 0.209054. Loss: 2.330147. Batch_acc: 0.252009. Batch_loss: 2.162056 \n",
      "Batch: 457. Acc: 0.209162. Loss: 2.329731. Batch_acc: 0.257679. Batch_loss: 2.141912 \n",
      "Batch: 458. Acc: 0.209247. Loss: 2.329343. Batch_acc: 0.247458. Batch_loss: 2.154711 \n",
      "Batch: 459. Acc: 0.209349. Loss: 2.328949. Batch_acc: 0.256160. Batch_loss: 2.149259 \n",
      "Batch: 460. Acc: 0.209442. Loss: 2.328556. Batch_acc: 0.253231. Batch_loss: 2.143775 \n",
      "Batch: 461. Acc: 0.209530. Loss: 2.328193. Batch_acc: 0.248879. Batch_loss: 2.165114 \n",
      "Batch: 462. Acc: 0.209618. Loss: 2.327791. Batch_acc: 0.249857. Batch_loss: 2.142692 \n",
      "Batch: 463. Acc: 0.209726. Loss: 2.327415. Batch_acc: 0.260242. Batch_loss: 2.153180 \n",
      "Batch: 464. Acc: 0.209829. Loss: 2.327076. Batch_acc: 0.257454. Batch_loss: 2.170009 \n",
      "Batch: 465. Acc: 0.209960. Loss: 2.326633. Batch_acc: 0.270581. Batch_loss: 2.120918 \n",
      "Batch: 466. Acc: 0.210000. Loss: 2.326342. Batch_acc: 0.229070. Batch_loss: 2.189152 \n",
      "Batch: 467. Acc: 0.210061. Loss: 2.326070. Batch_acc: 0.238841. Batch_loss: 2.197993 \n",
      "Batch: 468. Acc: 0.210140. Loss: 2.325722. Batch_acc: 0.247944. Batch_loss: 2.159389 \n",
      "Batch: 469. Acc: 0.210231. Loss: 2.325352. Batch_acc: 0.252589. Batch_loss: 2.151996 \n",
      "Batch: 470. Acc: 0.210317. Loss: 2.324991. Batch_acc: 0.251756. Batch_loss: 2.152545 \n",
      "Batch: 471. Acc: 0.210407. Loss: 2.324635. Batch_acc: 0.252595. Batch_loss: 2.156414 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 472. Acc: 0.210488. Loss: 2.324299. Batch_acc: 0.250000. Batch_loss: 2.161367 \n",
      "Batch: 473. Acc: 0.210572. Loss: 2.324011. Batch_acc: 0.252263. Batch_loss: 2.181156 \n",
      "Batch: 474. Acc: 0.210664. Loss: 2.323607. Batch_acc: 0.254169. Batch_loss: 2.132104 \n",
      "Batch: 475. Acc: 0.210753. Loss: 2.323249. Batch_acc: 0.251973. Batch_loss: 2.156819 \n",
      "Batch: 476. Acc: 0.210853. Loss: 2.322837. Batch_acc: 0.257919. Batch_loss: 2.130198 \n",
      "Batch: 477. Acc: 0.210953. Loss: 2.322461. Batch_acc: 0.257336. Batch_loss: 2.146707 \n",
      "Batch: 478. Acc: 0.211045. Loss: 2.322093. Batch_acc: 0.255014. Batch_loss: 2.146756 \n",
      "Batch: 479. Acc: 0.211136. Loss: 2.321742. Batch_acc: 0.254902. Batch_loss: 2.153474 \n",
      "Batch: 480. Acc: 0.211245. Loss: 2.321342. Batch_acc: 0.264294. Batch_loss: 2.126476 \n",
      "Batch: 481. Acc: 0.211315. Loss: 2.321013. Batch_acc: 0.244571. Batch_loss: 2.163937 \n",
      "Batch: 482. Acc: 0.211407. Loss: 2.320647. Batch_acc: 0.255575. Batch_loss: 2.145590 \n",
      "Batch: 483. Acc: 0.211486. Loss: 2.320326. Batch_acc: 0.250000. Batch_loss: 2.163229 \n",
      "Batch: 484. Acc: 0.211551. Loss: 2.320032. Batch_acc: 0.243590. Batch_loss: 2.175896 \n",
      "Batch: 485. Acc: 0.211601. Loss: 2.319727. Batch_acc: 0.235530. Batch_loss: 2.172383 \n",
      "Batch: 486. Acc: 0.211701. Loss: 2.319338. Batch_acc: 0.259134. Batch_loss: 2.134605 \n",
      "Batch: 487. Acc: 0.211793. Loss: 2.318972. Batch_acc: 0.256780. Batch_loss: 2.140116 \n",
      "Batch: 488. Acc: 0.211916. Loss: 2.318557. Batch_acc: 0.272046. Batch_loss: 2.115710 \n",
      "Batch: 489. Acc: 0.211955. Loss: 2.318245. Batch_acc: 0.231352. Batch_loss: 2.163939 \n",
      "Batch: 490. Acc: 0.212045. Loss: 2.317864. Batch_acc: 0.255499. Batch_loss: 2.135080 \n",
      "Batch: 491. Acc: 0.212144. Loss: 2.317497. Batch_acc: 0.260870. Batch_loss: 2.135935 \n",
      "Batch: 492. Acc: 0.212260. Loss: 2.317126. Batch_acc: 0.270035. Batch_loss: 2.132728 \n",
      "Batch: 493. Acc: 0.212326. Loss: 2.316814. Batch_acc: 0.244193. Batch_loss: 2.165566 \n",
      "Batch: 494. Acc: 0.212427. Loss: 2.316456. Batch_acc: 0.262041. Batch_loss: 2.140312 \n",
      "Batch: 495. Acc: 0.212528. Loss: 2.316114. Batch_acc: 0.262342. Batch_loss: 2.146924 \n",
      "Batch: 496. Acc: 0.212638. Loss: 2.315722. Batch_acc: 0.266366. Batch_loss: 2.124995 \n",
      "Batch: 497. Acc: 0.212734. Loss: 2.315365. Batch_acc: 0.262492. Batch_loss: 2.129904 \n",
      "Batch: 498. Acc: 0.212833. Loss: 2.315048. Batch_acc: 0.262164. Batch_loss: 2.158083 \n",
      "Batch: 499. Acc: 0.212896. Loss: 2.314736. Batch_acc: 0.243875. Batch_loss: 2.160446 \n",
      "Batch: 500. Acc: 0.212946. Loss: 2.314404. Batch_acc: 0.238234. Batch_loss: 2.147032 \n",
      "Batch: 501. Acc: 0.213014. Loss: 2.314081. Batch_acc: 0.247393. Batch_loss: 2.151069 \n",
      "Batch: 502. Acc: 0.213126. Loss: 2.313665. Batch_acc: 0.269231. Batch_loss: 2.105406 \n",
      "Checkpointing on batch: 502. Accuracy: 0.2131263829680124. Loss per char: 2.313665134014177. Time: 1627203002.3915946\n",
      "Last question is tensor([ 2, 49, 86, 85,  1, 85, 80, 72, 70, 85, 73, 70, 83,  1, 14, 22, 19, 15,\n",
      "        25, 20, 18,  1, 66, 79, 69,  1, 14, 18, 26, 20, 15, 18, 18, 25, 23, 23,\n",
      "        15,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0], device='cuda:0')\n",
      "Removing existing model file at checkpoints\\add_or_sub-2021-07-25_latest_checkpoint.pth\n",
      "Starting checkpoint save of checkpoints\\add_or_sub-2021-07-25_latest_checkpoint.pth...\n",
      "Final saved model size: 530790651\n",
      "Batch: 503. Acc: 0.213236. Loss: 2.313268. Batch_acc: 0.267304. Batch_loss: 2.118068 \n",
      "Batch: 504. Acc: 0.213331. Loss: 2.312944. Batch_acc: 0.261298. Batch_loss: 2.148232 \n",
      "Batch: 505. Acc: 0.213416. Loss: 2.312638. Batch_acc: 0.257396. Batch_loss: 2.154141 \n",
      "Batch: 506. Acc: 0.213506. Loss: 2.312334. Batch_acc: 0.259733. Batch_loss: 2.156850 \n",
      "Batch: 507. Acc: 0.213585. Loss: 2.311994. Batch_acc: 0.253931. Batch_loss: 2.137463 \n",
      "Batch: 508. Acc: 0.213685. Loss: 2.311640. Batch_acc: 0.263631. Batch_loss: 2.136045 \n",
      "Batch: 509. Acc: 0.213774. Loss: 2.311249. Batch_acc: 0.260000. Batch_loss: 2.107575 \n",
      "Batch: 510. Acc: 0.213841. Loss: 2.310908. Batch_acc: 0.247452. Batch_loss: 2.140154 \n",
      "Batch: 511. Acc: 0.213944. Loss: 2.310579. Batch_acc: 0.267092. Batch_loss: 2.141460 \n",
      "Batch: 512. Acc: 0.214049. Loss: 2.310219. Batch_acc: 0.267898. Batch_loss: 2.124937 \n",
      "Batch: 513. Acc: 0.214158. Loss: 2.309847. Batch_acc: 0.270161. Batch_loss: 2.118940 \n",
      "Batch: 514. Acc: 0.214279. Loss: 2.309443. Batch_acc: 0.276460. Batch_loss: 2.100624 \n",
      "Batch: 515. Acc: 0.214362. Loss: 2.309117. Batch_acc: 0.255866. Batch_loss: 2.146200 \n",
      "Batch: 516. Acc: 0.214483. Loss: 2.308802. Batch_acc: 0.277778. Batch_loss: 2.143844 \n",
      "Batch: 517. Acc: 0.214556. Loss: 2.308462. Batch_acc: 0.251843. Batch_loss: 2.134989 \n",
      "Batch: 518. Acc: 0.214662. Loss: 2.308067. Batch_acc: 0.269605. Batch_loss: 2.104870 \n",
      "Batch: 519. Acc: 0.214743. Loss: 2.307691. Batch_acc: 0.256586. Batch_loss: 2.113293 \n",
      "Batch: 520. Acc: 0.214830. Loss: 2.307345. Batch_acc: 0.258640. Batch_loss: 2.133070 \n",
      "Batch: 521. Acc: 0.214886. Loss: 2.307060. Batch_acc: 0.244226. Batch_loss: 2.157923 \n",
      "Batch: 522. Acc: 0.214982. Loss: 2.306745. Batch_acc: 0.263601. Batch_loss: 2.146598 \n",
      "Batch: 523. Acc: 0.215051. Loss: 2.306487. Batch_acc: 0.251455. Batch_loss: 2.170248 \n",
      "Batch: 524. Acc: 0.215138. Loss: 2.306204. Batch_acc: 0.260057. Batch_loss: 2.160003 \n",
      "Batch: 525. Acc: 0.215231. Loss: 2.305877. Batch_acc: 0.263188. Batch_loss: 2.136812 \n",
      "Batch: 526. Acc: 0.215306. Loss: 2.305572. Batch_acc: 0.254846. Batch_loss: 2.146724 \n",
      "Batch: 527. Acc: 0.215411. Loss: 2.305214. Batch_acc: 0.269778. Batch_loss: 2.118102 \n",
      "Batch: 528. Acc: 0.215483. Loss: 2.304915. Batch_acc: 0.253913. Batch_loss: 2.146285 \n",
      "Batch: 529. Acc: 0.215582. Loss: 2.304541. Batch_acc: 0.267816. Batch_loss: 2.106620 \n",
      "Batch: 530. Acc: 0.215674. Loss: 2.304227. Batch_acc: 0.262867. Batch_loss: 2.144148 \n",
      "Batch: 531. Acc: 0.215773. Loss: 2.303895. Batch_acc: 0.268237. Batch_loss: 2.128142 \n",
      "Batch: 532. Acc: 0.215872. Loss: 2.303566. Batch_acc: 0.269074. Batch_loss: 2.126187 \n",
      "Batch: 533. Acc: 0.215947. Loss: 2.303270. Batch_acc: 0.256041. Batch_loss: 2.145430 \n",
      "Batch: 534. Acc: 0.216041. Loss: 2.302930. Batch_acc: 0.266129. Batch_loss: 2.121583 \n",
      "Batch: 535. Acc: 0.216151. Loss: 2.302556. Batch_acc: 0.274019. Batch_loss: 2.104735 \n",
      "Batch: 536. Acc: 0.216252. Loss: 2.302254. Batch_acc: 0.272565. Batch_loss: 2.134883 \n",
      "Batch: 537. Acc: 0.216289. Loss: 2.302034. Batch_acc: 0.236248. Batch_loss: 2.183050 \n",
      "Batch: 538. Acc: 0.216376. Loss: 2.301715. Batch_acc: 0.262407. Batch_loss: 2.131879 \n",
      "Batch: 539. Acc: 0.216467. Loss: 2.301343. Batch_acc: 0.265074. Batch_loss: 2.102894 \n",
      "Batch: 540. Acc: 0.216541. Loss: 2.301070. Batch_acc: 0.257817. Batch_loss: 2.149871 \n",
      "Batch: 541. Acc: 0.216630. Loss: 2.300761. Batch_acc: 0.265342. Batch_loss: 2.131183 \n",
      "Batch: 542. Acc: 0.216708. Loss: 2.300456. Batch_acc: 0.257778. Batch_loss: 2.140626 \n",
      "Batch: 543. Acc: 0.216804. Loss: 2.300119. Batch_acc: 0.268223. Batch_loss: 2.119223 \n",
      "Batch: 544. Acc: 0.216909. Loss: 2.299768. Batch_acc: 0.273980. Batch_loss: 2.108925 \n",
      "Batch: 545. Acc: 0.216977. Loss: 2.299475. Batch_acc: 0.254366. Batch_loss: 2.138308 \n",
      "Batch: 546. Acc: 0.217050. Loss: 2.299163. Batch_acc: 0.257780. Batch_loss: 2.125338 \n",
      "Batch: 547. Acc: 0.217136. Loss: 2.298855. Batch_acc: 0.264757. Batch_loss: 2.127391 \n",
      "Batch: 548. Acc: 0.217219. Loss: 2.298561. Batch_acc: 0.263097. Batch_loss: 2.135640 \n",
      "Batch: 549. Acc: 0.217309. Loss: 2.298280. Batch_acc: 0.266974. Batch_loss: 2.144313 \n",
      "Batch: 550. Acc: 0.217411. Loss: 2.297958. Batch_acc: 0.273356. Batch_loss: 2.120472 \n",
      "Batch: 551. Acc: 0.217501. Loss: 2.297589. Batch_acc: 0.266553. Batch_loss: 2.097649 \n",
      "Batch: 552. Acc: 0.217589. Loss: 2.297240. Batch_acc: 0.265387. Batch_loss: 2.107970 \n",
      "Batch: 553. Acc: 0.217688. Loss: 2.296869. Batch_acc: 0.272412. Batch_loss: 2.090873 \n",
      "Batch: 554. Acc: 0.217766. Loss: 2.296550. Batch_acc: 0.261710. Batch_loss: 2.116396 \n",
      "Batch: 555. Acc: 0.217846. Loss: 2.296260. Batch_acc: 0.261986. Batch_loss: 2.136552 \n",
      "Batch: 556. Acc: 0.217910. Loss: 2.295949. Batch_acc: 0.254396. Batch_loss: 2.119776 \n",
      "Batch: 557. Acc: 0.217976. Loss: 2.295735. Batch_acc: 0.255108. Batch_loss: 2.174956 \n",
      "Batch: 558. Acc: 0.218079. Loss: 2.295364. Batch_acc: 0.274321. Batch_loss: 2.091714 \n",
      "Batch: 559. Acc: 0.218153. Loss: 2.295075. Batch_acc: 0.260290. Batch_loss: 2.132426 \n",
      "Batch: 560. Acc: 0.218245. Loss: 2.294767. Batch_acc: 0.268623. Batch_loss: 2.125698 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 561. Acc: 0.218310. Loss: 2.294466. Batch_acc: 0.254296. Batch_loss: 2.126526 \n",
      "Batch: 562. Acc: 0.218402. Loss: 2.294128. Batch_acc: 0.270930. Batch_loss: 2.101886 \n",
      "Batch: 563. Acc: 0.218471. Loss: 2.293828. Batch_acc: 0.258560. Batch_loss: 2.120948 \n",
      "Batch: 564. Acc: 0.218547. Loss: 2.293519. Batch_acc: 0.261547. Batch_loss: 2.118165 \n",
      "Batch: 565. Acc: 0.218633. Loss: 2.293194. Batch_acc: 0.266138. Batch_loss: 2.112931 \n",
      "Batch: 566. Acc: 0.218732. Loss: 2.292864. Batch_acc: 0.276084. Batch_loss: 2.102225 \n",
      "Batch: 567. Acc: 0.218814. Loss: 2.292552. Batch_acc: 0.266588. Batch_loss: 2.110359 \n",
      "Batch: 568. Acc: 0.218919. Loss: 2.292235. Batch_acc: 0.278286. Batch_loss: 2.113920 \n",
      "Batch: 569. Acc: 0.218995. Loss: 2.291904. Batch_acc: 0.262881. Batch_loss: 2.100243 \n",
      "Batch: 570. Acc: 0.219071. Loss: 2.291575. Batch_acc: 0.260966. Batch_loss: 2.110368 \n",
      "Batch: 571. Acc: 0.219146. Loss: 2.291253. Batch_acc: 0.262029. Batch_loss: 2.106079 \n",
      "Batch: 572. Acc: 0.219239. Loss: 2.290921. Batch_acc: 0.272727. Batch_loss: 2.099806 \n",
      "Batch: 573. Acc: 0.219280. Loss: 2.290666. Batch_acc: 0.243402. Batch_loss: 2.141893 \n",
      "Batch: 574. Acc: 0.219348. Loss: 2.290412. Batch_acc: 0.258440. Batch_loss: 2.142771 \n",
      "Batch: 575. Acc: 0.219423. Loss: 2.290126. Batch_acc: 0.263620. Batch_loss: 2.122588 \n",
      "Batch: 576. Acc: 0.219496. Loss: 2.289849. Batch_acc: 0.261521. Batch_loss: 2.130490 \n",
      "Batch: 577. Acc: 0.219568. Loss: 2.289580. Batch_acc: 0.261529. Batch_loss: 2.131835 \n",
      "Batch: 578. Acc: 0.219654. Loss: 2.289256. Batch_acc: 0.268087. Batch_loss: 2.106996 \n",
      "Batch: 579. Acc: 0.219756. Loss: 2.288939. Batch_acc: 0.277809. Batch_loss: 2.108551 \n",
      "Batch: 580. Acc: 0.219842. Loss: 2.288639. Batch_acc: 0.269832. Batch_loss: 2.114046 \n",
      "Batch: 581. Acc: 0.219919. Loss: 2.288335. Batch_acc: 0.265165. Batch_loss: 2.110606 \n",
      "Batch: 582. Acc: 0.220001. Loss: 2.288093. Batch_acc: 0.269529. Batch_loss: 2.142524 \n",
      "Batch: 583. Acc: 0.220074. Loss: 2.287826. Batch_acc: 0.262136. Batch_loss: 2.133124 \n",
      "Batch: 584. Acc: 0.220174. Loss: 2.287492. Batch_acc: 0.277339. Batch_loss: 2.096691 \n",
      "Batch: 585. Acc: 0.220274. Loss: 2.287135. Batch_acc: 0.279056. Batch_loss: 2.078327 \n",
      "Batch: 586. Acc: 0.220343. Loss: 2.286871. Batch_acc: 0.260669. Batch_loss: 2.131624 \n",
      "Batch: 587. Acc: 0.220425. Loss: 2.286536. Batch_acc: 0.268391. Batch_loss: 2.089979 \n",
      "Batch: 588. Acc: 0.220504. Loss: 2.286253. Batch_acc: 0.267401. Batch_loss: 2.118444 \n",
      "Batch: 589. Acc: 0.220577. Loss: 2.285959. Batch_acc: 0.263615. Batch_loss: 2.112087 \n",
      "Batch: 590. Acc: 0.220642. Loss: 2.285671. Batch_acc: 0.260306. Batch_loss: 2.111866 \n",
      "Batch: 591. Acc: 0.220725. Loss: 2.285392. Batch_acc: 0.269297. Batch_loss: 2.121243 \n",
      "Batch: 592. Acc: 0.220822. Loss: 2.285084. Batch_acc: 0.278226. Batch_loss: 2.102795 \n",
      "Batch: 593. Acc: 0.220902. Loss: 2.284808. Batch_acc: 0.269321. Batch_loss: 2.118465 \n",
      "Batch: 594. Acc: 0.221014. Loss: 2.284467. Batch_acc: 0.284845. Batch_loss: 2.089528 \n",
      "Batch: 595. Acc: 0.221074. Loss: 2.284235. Batch_acc: 0.257780. Batch_loss: 2.143632 \n",
      "Batch: 596. Acc: 0.221132. Loss: 2.284020. Batch_acc: 0.256903. Batch_loss: 2.150301 \n",
      "Batch: 597. Acc: 0.221227. Loss: 2.283711. Batch_acc: 0.277937. Batch_loss: 2.099725 \n",
      "Batch: 598. Acc: 0.221317. Loss: 2.283394. Batch_acc: 0.274011. Batch_loss: 2.097207 \n",
      "Batch: 599. Acc: 0.221402. Loss: 2.283090. Batch_acc: 0.273208. Batch_loss: 2.097561 \n",
      "Batch: 600. Acc: 0.221489. Loss: 2.282838. Batch_acc: 0.274249. Batch_loss: 2.131261 \n",
      "Batch: 601. Acc: 0.221586. Loss: 2.282530. Batch_acc: 0.278754. Batch_loss: 2.100021 \n",
      "Batch: 602. Acc: 0.221672. Loss: 2.282244. Batch_acc: 0.273303. Batch_loss: 2.110115 \n",
      "Batch: 603. Acc: 0.221762. Loss: 2.281933. Batch_acc: 0.273678. Batch_loss: 2.102429 \n",
      "Batch: 604. Acc: 0.221846. Loss: 2.281648. Batch_acc: 0.272884. Batch_loss: 2.109610 \n",
      "Batch: 605. Acc: 0.221939. Loss: 2.281330. Batch_acc: 0.277937. Batch_loss: 2.089566 \n",
      "Batch: 606. Acc: 0.222016. Loss: 2.281078. Batch_acc: 0.269388. Batch_loss: 2.126467 \n",
      "Batch: 607. Acc: 0.222087. Loss: 2.280819. Batch_acc: 0.265461. Batch_loss: 2.121766 \n",
      "Batch: 608. Acc: 0.222156. Loss: 2.280583. Batch_acc: 0.264282. Batch_loss: 2.136383 \n",
      "Batch: 609. Acc: 0.222209. Loss: 2.280274. Batch_acc: 0.253363. Batch_loss: 2.097373 \n",
      "Batch: 610. Acc: 0.222299. Loss: 2.279973. Batch_acc: 0.277019. Batch_loss: 2.098199 \n",
      "Batch: 611. Acc: 0.222389. Loss: 2.279666. Batch_acc: 0.276644. Batch_loss: 2.094728 \n",
      "Batch: 612. Acc: 0.222475. Loss: 2.279347. Batch_acc: 0.273649. Batch_loss: 2.088747 \n",
      "Batch: 613. Acc: 0.222531. Loss: 2.279041. Batch_acc: 0.256483. Batch_loss: 2.095077 \n",
      "Batch: 614. Acc: 0.222620. Loss: 2.278739. Batch_acc: 0.276888. Batch_loss: 2.094174 \n",
      "Batch: 615. Acc: 0.222714. Loss: 2.278420. Batch_acc: 0.280229. Batch_loss: 2.083039 \n",
      "Batch: 616. Acc: 0.222801. Loss: 2.278082. Batch_acc: 0.276932. Batch_loss: 2.066508 \n",
      "Batch: 617. Acc: 0.222893. Loss: 2.277788. Batch_acc: 0.278339. Batch_loss: 2.101073 \n",
      "Batch: 618. Acc: 0.222974. Loss: 2.277492. Batch_acc: 0.272831. Batch_loss: 2.095881 \n",
      "Batch: 619. Acc: 0.223051. Loss: 2.277225. Batch_acc: 0.271147. Batch_loss: 2.110615 \n",
      "Batch: 620. Acc: 0.223145. Loss: 2.276915. Batch_acc: 0.280226. Batch_loss: 2.088111 \n",
      "Batch: 621. Acc: 0.223221. Loss: 2.276593. Batch_acc: 0.269404. Batch_loss: 2.081339 \n",
      "Batch: 622. Acc: 0.223289. Loss: 2.276337. Batch_acc: 0.265040. Batch_loss: 2.119352 \n",
      "Batch: 623. Acc: 0.223378. Loss: 2.276035. Batch_acc: 0.277035. Batch_loss: 2.093476 \n",
      "Batch: 624. Acc: 0.223442. Loss: 2.275782. Batch_acc: 0.262472. Batch_loss: 2.120248 \n",
      "Batch: 625. Acc: 0.223541. Loss: 2.275494. Batch_acc: 0.285386. Batch_loss: 2.095472 \n",
      "Batch: 626. Acc: 0.223622. Loss: 2.275215. Batch_acc: 0.274600. Batch_loss: 2.102057 \n",
      "Batch: 627. Acc: 0.223680. Loss: 2.274975. Batch_acc: 0.259861. Batch_loss: 2.123160 \n",
      "Batch: 628. Acc: 0.223749. Loss: 2.274685. Batch_acc: 0.266591. Batch_loss: 2.095163 \n",
      "Batch: 629. Acc: 0.223817. Loss: 2.274482. Batch_acc: 0.267211. Batch_loss: 2.144659 \n",
      "Batch: 630. Acc: 0.223883. Loss: 2.274224. Batch_acc: 0.265446. Batch_loss: 2.112430 \n",
      "Batch: 631. Acc: 0.223950. Loss: 2.274006. Batch_acc: 0.265643. Batch_loss: 2.138239 \n",
      "Batch: 632. Acc: 0.224030. Loss: 2.273701. Batch_acc: 0.274175. Batch_loss: 2.083315 \n",
      "Batch: 633. Acc: 0.224089. Loss: 2.273465. Batch_acc: 0.261529. Batch_loss: 2.121941 \n",
      "Batch: 634. Acc: 0.224156. Loss: 2.273187. Batch_acc: 0.266179. Batch_loss: 2.100368 \n",
      "Batch: 635. Acc: 0.224218. Loss: 2.272910. Batch_acc: 0.264173. Batch_loss: 2.094414 \n",
      "Batch: 636. Acc: 0.224289. Loss: 2.272679. Batch_acc: 0.270334. Batch_loss: 2.123426 \n",
      "Batch: 637. Acc: 0.224387. Loss: 2.272419. Batch_acc: 0.286790. Batch_loss: 2.105589 \n",
      "Batch: 638. Acc: 0.224449. Loss: 2.272177. Batch_acc: 0.264335. Batch_loss: 2.118158 \n",
      "Batch: 639. Acc: 0.224506. Loss: 2.271933. Batch_acc: 0.260495. Batch_loss: 2.116027 \n",
      "Batch: 640. Acc: 0.224592. Loss: 2.271632. Batch_acc: 0.278305. Batch_loss: 2.085022 \n",
      "Batch: 641. Acc: 0.224684. Loss: 2.271367. Batch_acc: 0.285292. Batch_loss: 2.096936 \n",
      "Batch: 642. Acc: 0.224759. Loss: 2.271095. Batch_acc: 0.272105. Batch_loss: 2.097854 \n",
      "Batch: 643. Acc: 0.224800. Loss: 2.270849. Batch_acc: 0.250423. Batch_loss: 2.115559 \n",
      "Batch: 644. Acc: 0.224864. Loss: 2.270598. Batch_acc: 0.266361. Batch_loss: 2.109341 \n",
      "Batch: 645. Acc: 0.224954. Loss: 2.270310. Batch_acc: 0.282534. Batch_loss: 2.086500 \n",
      "Batch: 646. Acc: 0.225028. Loss: 2.270024. Batch_acc: 0.273095. Batch_loss: 2.084306 \n",
      "Batch: 647. Acc: 0.225111. Loss: 2.269752. Batch_acc: 0.278539. Batch_loss: 2.095156 \n",
      "Batch: 648. Acc: 0.225195. Loss: 2.269470. Batch_acc: 0.279403. Batch_loss: 2.087197 \n",
      "Batch: 649. Acc: 0.225248. Loss: 2.269262. Batch_acc: 0.259977. Batch_loss: 2.131362 \n",
      "Batch: 650. Acc: 0.225303. Loss: 2.268992. Batch_acc: 0.261095. Batch_loss: 2.093399 \n",
      "Batch: 651. Acc: 0.225392. Loss: 2.268685. Batch_acc: 0.282109. Batch_loss: 2.073644 \n",
      "Batch: 652. Acc: 0.225450. Loss: 2.268434. Batch_acc: 0.263339. Batch_loss: 2.105393 \n",
      "Batch: 653. Acc: 0.225522. Loss: 2.268204. Batch_acc: 0.273154. Batch_loss: 2.115217 \n",
      "Batch: 654. Acc: 0.225590. Loss: 2.267940. Batch_acc: 0.270459. Batch_loss: 2.093983 \n",
      "Batch: 655. Acc: 0.225682. Loss: 2.267651. Batch_acc: 0.286043. Batch_loss: 2.078542 \n",
      "Batch: 656. Acc: 0.225771. Loss: 2.267364. Batch_acc: 0.283126. Batch_loss: 2.081764 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 657. Acc: 0.225811. Loss: 2.267155. Batch_acc: 0.252018. Batch_loss: 2.129925 \n",
      "Batch: 658. Acc: 0.225866. Loss: 2.266922. Batch_acc: 0.262727. Batch_loss: 2.110526 \n",
      "Batch: 659. Acc: 0.225944. Loss: 2.266660. Batch_acc: 0.276427. Batch_loss: 2.097258 \n",
      "Batch: 660. Acc: 0.226010. Loss: 2.266436. Batch_acc: 0.269099. Batch_loss: 2.119660 \n",
      "Batch: 661. Acc: 0.226086. Loss: 2.266170. Batch_acc: 0.276278. Batch_loss: 2.090709 \n",
      "Batch: 662. Acc: 0.226144. Loss: 2.265948. Batch_acc: 0.265838. Batch_loss: 2.114531 \n",
      "Batch: 663. Acc: 0.226188. Loss: 2.265738. Batch_acc: 0.255828. Batch_loss: 2.124950 \n",
      "Batch: 664. Acc: 0.226259. Loss: 2.265504. Batch_acc: 0.273623. Batch_loss: 2.108867 \n",
      "Batch: 665. Acc: 0.226338. Loss: 2.265247. Batch_acc: 0.278868. Batch_loss: 2.094066 \n",
      "Batch: 666. Acc: 0.226425. Loss: 2.264975. Batch_acc: 0.282755. Batch_loss: 2.088660 \n",
      "Batch: 667. Acc: 0.226476. Loss: 2.264773. Batch_acc: 0.260672. Batch_loss: 2.131520 \n",
      "Batch: 668. Acc: 0.226535. Loss: 2.264579. Batch_acc: 0.265436. Batch_loss: 2.134424 \n",
      "Batch: 669. Acc: 0.226610. Loss: 2.264309. Batch_acc: 0.277233. Batch_loss: 2.083464 \n",
      "Batch: 670. Acc: 0.226676. Loss: 2.264056. Batch_acc: 0.270785. Batch_loss: 2.093863 \n",
      "Batch: 671. Acc: 0.226742. Loss: 2.263811. Batch_acc: 0.271619. Batch_loss: 2.097543 \n",
      "Batch: 672. Acc: 0.226783. Loss: 2.263631. Batch_acc: 0.254619. Batch_loss: 2.142220 \n",
      "Batch: 673. Acc: 0.226848. Loss: 2.263436. Batch_acc: 0.269252. Batch_loss: 2.135445 \n",
      "Batch: 674. Acc: 0.226926. Loss: 2.263166. Batch_acc: 0.279907. Batch_loss: 2.079508 \n",
      "Batch: 675. Acc: 0.227002. Loss: 2.262947. Batch_acc: 0.277778. Batch_loss: 2.117080 \n",
      "Batch: 676. Acc: 0.227061. Loss: 2.262722. Batch_acc: 0.267281. Batch_loss: 2.110816 \n",
      "Batch: 677. Acc: 0.227123. Loss: 2.262492. Batch_acc: 0.268640. Batch_loss: 2.108225 \n",
      "Batch: 678. Acc: 0.227165. Loss: 2.262251. Batch_acc: 0.255172. Batch_loss: 2.099221 \n",
      "Batch: 679. Acc: 0.227222. Loss: 2.262030. Batch_acc: 0.266436. Batch_loss: 2.111048 \n",
      "Batch: 680. Acc: 0.227294. Loss: 2.261784. Batch_acc: 0.276896. Batch_loss: 2.090968 \n",
      "Batch: 681. Acc: 0.227366. Loss: 2.261503. Batch_acc: 0.274017. Batch_loss: 2.080098 \n",
      "Batch: 682. Acc: 0.227465. Loss: 2.261233. Batch_acc: 0.293984. Batch_loss: 2.079873 \n",
      "Batch: 683. Acc: 0.227532. Loss: 2.261017. Batch_acc: 0.274985. Batch_loss: 2.109069 \n",
      "Batch: 684. Acc: 0.227597. Loss: 2.260758. Batch_acc: 0.272994. Batch_loss: 2.080232 \n",
      "Batch: 685. Acc: 0.227670. Loss: 2.260515. Batch_acc: 0.277940. Batch_loss: 2.090962 \n",
      "Batch: 686. Acc: 0.227718. Loss: 2.260298. Batch_acc: 0.259610. Batch_loss: 2.116160 \n",
      "Batch: 687. Acc: 0.227791. Loss: 2.260066. Batch_acc: 0.277904. Batch_loss: 2.102764 \n",
      "Batch: 688. Acc: 0.227850. Loss: 2.259856. Batch_acc: 0.268849. Batch_loss: 2.113120 \n",
      "Batch: 689. Acc: 0.227929. Loss: 2.259609. Batch_acc: 0.282496. Batch_loss: 2.088217 \n",
      "Batch: 690. Acc: 0.227984. Loss: 2.259379. Batch_acc: 0.265982. Batch_loss: 2.102043 \n",
      "Batch: 691. Acc: 0.228059. Loss: 2.259126. Batch_acc: 0.279403. Batch_loss: 2.084841 \n",
      "Batch: 692. Acc: 0.228125. Loss: 2.258876. Batch_acc: 0.275431. Batch_loss: 2.080244 \n",
      "Batch: 693. Acc: 0.228200. Loss: 2.258646. Batch_acc: 0.282297. Batch_loss: 2.092405 \n",
      "Batch: 694. Acc: 0.228272. Loss: 2.258388. Batch_acc: 0.278063. Batch_loss: 2.081504 \n",
      "Batch: 695. Acc: 0.228350. Loss: 2.258137. Batch_acc: 0.282272. Batch_loss: 2.084126 \n",
      "Batch: 696. Acc: 0.228415. Loss: 2.257931. Batch_acc: 0.274336. Batch_loss: 2.110725 \n",
      "Batch: 697. Acc: 0.228467. Loss: 2.257725. Batch_acc: 0.265414. Batch_loss: 2.111432 \n",
      "Batch: 698. Acc: 0.228523. Loss: 2.257514. Batch_acc: 0.268279. Batch_loss: 2.110269 \n",
      "Batch: 699. Acc: 0.228602. Loss: 2.257276. Batch_acc: 0.284302. Batch_loss: 2.088948 \n",
      "Batch: 700. Acc: 0.228670. Loss: 2.257066. Batch_acc: 0.275982. Batch_loss: 2.109428 \n",
      "Batch: 701. Acc: 0.228706. Loss: 2.256869. Batch_acc: 0.254023. Batch_loss: 2.118796 \n",
      "Batch: 702. Acc: 0.228774. Loss: 2.256622. Batch_acc: 0.276413. Batch_loss: 2.085003 \n",
      "Batch: 703. Acc: 0.228828. Loss: 2.256382. Batch_acc: 0.267333. Batch_loss: 2.083725 \n",
      "Batch: 704. Acc: 0.228885. Loss: 2.256159. Batch_acc: 0.269364. Batch_loss: 2.098138 \n",
      "Batch: 705. Acc: 0.228950. Loss: 2.255932. Batch_acc: 0.275043. Batch_loss: 2.095537 \n",
      "Batch: 706. Acc: 0.229028. Loss: 2.255672. Batch_acc: 0.283901. Batch_loss: 2.071574 \n",
      "Batch: 707. Acc: 0.229094. Loss: 2.255424. Batch_acc: 0.274554. Batch_loss: 2.084819 \n",
      "Batch: 708. Acc: 0.229172. Loss: 2.255181. Batch_acc: 0.284483. Batch_loss: 2.083554 \n",
      "Batch: 709. Acc: 0.229240. Loss: 2.254937. Batch_acc: 0.276897. Batch_loss: 2.084539 \n",
      "Batch: 710. Acc: 0.229350. Loss: 2.254617. Batch_acc: 0.305151. Batch_loss: 2.033893 \n",
      "Batch: 711. Acc: 0.229376. Loss: 2.254426. Batch_acc: 0.247953. Batch_loss: 2.116113 \n",
      "Batch: 712. Acc: 0.229449. Loss: 2.254156. Batch_acc: 0.281161. Batch_loss: 2.064321 \n",
      "Batch: 713. Acc: 0.229472. Loss: 2.254004. Batch_acc: 0.245748. Batch_loss: 2.142887 \n",
      "Batch: 714. Acc: 0.229543. Loss: 2.253748. Batch_acc: 0.279772. Batch_loss: 2.072892 \n",
      "Batch: 715. Acc: 0.229592. Loss: 2.253554. Batch_acc: 0.264790. Batch_loss: 2.115456 \n",
      "Batch: 716. Acc: 0.229659. Loss: 2.253338. Batch_acc: 0.276958. Batch_loss: 2.100288 \n",
      "Batch: 717. Acc: 0.229718. Loss: 2.253146. Batch_acc: 0.272516. Batch_loss: 2.114100 \n",
      "Batch: 718. Acc: 0.229789. Loss: 2.252906. Batch_acc: 0.281797. Batch_loss: 2.078201 \n",
      "Batch: 719. Acc: 0.229842. Loss: 2.252714. Batch_acc: 0.267581. Batch_loss: 2.115565 \n",
      "Batch: 720. Acc: 0.229908. Loss: 2.252488. Batch_acc: 0.275843. Batch_loss: 2.094090 \n",
      "Batch: 721. Acc: 0.229974. Loss: 2.252259. Batch_acc: 0.279248. Batch_loss: 2.083473 \n",
      "Batch: 722. Acc: 0.230041. Loss: 2.252030. Batch_acc: 0.278035. Batch_loss: 2.085413 \n",
      "Batch: 723. Acc: 0.230101. Loss: 2.251797. Batch_acc: 0.273618. Batch_loss: 2.083310 \n",
      "Batch: 724. Acc: 0.230173. Loss: 2.251522. Batch_acc: 0.281285. Batch_loss: 2.056509 \n",
      "Batch: 725. Acc: 0.230214. Loss: 2.251315. Batch_acc: 0.260102. Batch_loss: 2.103062 \n",
      "Batch: 726. Acc: 0.230297. Loss: 2.251066. Batch_acc: 0.290138. Batch_loss: 2.070784 \n",
      "Batch: 727. Acc: 0.230367. Loss: 2.250829. Batch_acc: 0.280963. Batch_loss: 2.079052 \n",
      "Batch: 728. Acc: 0.230452. Loss: 2.250596. Batch_acc: 0.292529. Batch_loss: 2.081088 \n",
      "Batch: 729. Acc: 0.230529. Loss: 2.250352. Batch_acc: 0.286705. Batch_loss: 2.071684 \n",
      "Batch: 730. Acc: 0.230582. Loss: 2.250137. Batch_acc: 0.269297. Batch_loss: 2.093814 \n",
      "Batch: 731. Acc: 0.230643. Loss: 2.249917. Batch_acc: 0.274276. Batch_loss: 2.091118 \n",
      "Batch: 732. Acc: 0.230715. Loss: 2.249676. Batch_acc: 0.283191. Batch_loss: 2.075228 \n",
      "Batch: 733. Acc: 0.230787. Loss: 2.249443. Batch_acc: 0.282880. Batch_loss: 2.081203 \n",
      "Batch: 734. Acc: 0.230830. Loss: 2.249249. Batch_acc: 0.262164. Batch_loss: 2.107533 \n",
      "Batch: 735. Acc: 0.230884. Loss: 2.249035. Batch_acc: 0.270534. Batch_loss: 2.092286 \n",
      "Batch: 736. Acc: 0.230935. Loss: 2.248831. Batch_acc: 0.268166. Batch_loss: 2.097712 \n",
      "Batch: 737. Acc: 0.230998. Loss: 2.248621. Batch_acc: 0.278555. Batch_loss: 2.091817 \n",
      "Batch: 738. Acc: 0.231056. Loss: 2.248415. Batch_acc: 0.274985. Batch_loss: 2.092383 \n",
      "Batch: 739. Acc: 0.231107. Loss: 2.248184. Batch_acc: 0.268571. Batch_loss: 2.078422 \n",
      "Batch: 740. Acc: 0.231166. Loss: 2.247966. Batch_acc: 0.273810. Batch_loss: 2.089545 \n",
      "Batch: 741. Acc: 0.231205. Loss: 2.247801. Batch_acc: 0.261025. Batch_loss: 2.120509 \n",
      "Batch: 742. Acc: 0.231255. Loss: 2.247595. Batch_acc: 0.268195. Batch_loss: 2.096026 \n",
      "Batch: 743. Acc: 0.231307. Loss: 2.247373. Batch_acc: 0.270426. Batch_loss: 2.082300 \n",
      "Batch: 744. Acc: 0.231367. Loss: 2.247170. Batch_acc: 0.275446. Batch_loss: 2.096073 \n",
      "Batch: 745. Acc: 0.231439. Loss: 2.246927. Batch_acc: 0.285714. Batch_loss: 2.065130 \n",
      "Batch: 746. Acc: 0.231484. Loss: 2.246742. Batch_acc: 0.266787. Batch_loss: 2.102360 \n",
      "Batch: 747. Acc: 0.231542. Loss: 2.246555. Batch_acc: 0.275924. Batch_loss: 2.101768 \n",
      "Batch: 748. Acc: 0.231610. Loss: 2.246373. Batch_acc: 0.281730. Batch_loss: 2.112295 \n",
      "Batch: 749. Acc: 0.231669. Loss: 2.246146. Batch_acc: 0.275470. Batch_loss: 2.077662 \n",
      "Batch: 750. Acc: 0.231728. Loss: 2.245966. Batch_acc: 0.276583. Batch_loss: 2.109774 \n",
      "Batch: 751. Acc: 0.231770. Loss: 2.245787. Batch_acc: 0.263218. Batch_loss: 2.111183 \n",
      "Batch: 752. Acc: 0.231835. Loss: 2.245543. Batch_acc: 0.280641. Batch_loss: 2.063263 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 753. Acc: 0.231863. Loss: 2.245381. Batch_acc: 0.252971. Batch_loss: 2.125078 \n",
      "Checkpointing on batch: 753. Accuracy: 0.2318634999168278. Loss per char: 2.2453808190020252. Time: 1627203181.4777737\n",
      "Last question is tensor([ 2, 56, 73, 66, 85,  1, 74, 84,  1, 19, 25, 23, 20, 19, 15, 21, 24,  1,\n",
      "        78, 74, 79, 86, 84,  1, 23, 19, 17, 23, 21, 18, 21, 22, 32,  3,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0], device='cuda:0')\n",
      "Removing existing model file at checkpoints\\add_or_sub-2021-07-25_latest_checkpoint.pth\n",
      "Starting checkpoint save of checkpoints\\add_or_sub-2021-07-25_latest_checkpoint.pth...\n",
      "Final saved model size: 530790651\n",
      "Batch: 754. Acc: 0.231917. Loss: 2.245205. Batch_acc: 0.272464. Batch_loss: 2.111274 \n",
      "Batch: 755. Acc: 0.231973. Loss: 2.245018. Batch_acc: 0.274453. Batch_loss: 2.104357 \n",
      "Batch: 756. Acc: 0.232041. Loss: 2.244793. Batch_acc: 0.282572. Batch_loss: 2.078081 \n",
      "Batch: 757. Acc: 0.232077. Loss: 2.244638. Batch_acc: 0.259174. Batch_loss: 2.127237 \n",
      "Batch: 758. Acc: 0.232141. Loss: 2.244435. Batch_acc: 0.281525. Batch_loss: 2.087933 \n",
      "Batch: 759. Acc: 0.232195. Loss: 2.244249. Batch_acc: 0.272416. Batch_loss: 2.103887 \n",
      "Batch: 760. Acc: 0.232248. Loss: 2.244051. Batch_acc: 0.271508. Batch_loss: 2.097733 \n",
      "Batch: 761. Acc: 0.232300. Loss: 2.243824. Batch_acc: 0.271696. Batch_loss: 2.073548 \n",
      "Batch: 762. Acc: 0.232351. Loss: 2.243610. Batch_acc: 0.271930. Batch_loss: 2.078017 \n",
      "Batch: 763. Acc: 0.232433. Loss: 2.243372. Batch_acc: 0.295271. Batch_loss: 2.061566 \n",
      "Batch: 764. Acc: 0.232497. Loss: 2.243146. Batch_acc: 0.280551. Batch_loss: 2.071088 \n",
      "Batch: 765. Acc: 0.232564. Loss: 2.242940. Batch_acc: 0.284880. Batch_loss: 2.082911 \n",
      "Batch: 766. Acc: 0.232611. Loss: 2.242774. Batch_acc: 0.269074. Batch_loss: 2.114166 \n",
      "Batch: 767. Acc: 0.232686. Loss: 2.242528. Batch_acc: 0.290583. Batch_loss: 2.053154 \n",
      "Batch: 768. Acc: 0.232748. Loss: 2.242332. Batch_acc: 0.280834. Batch_loss: 2.090774 \n",
      "Batch: 769. Acc: 0.232793. Loss: 2.242175. Batch_acc: 0.267049. Batch_loss: 2.121503 \n",
      "Batch: 770. Acc: 0.232861. Loss: 2.241987. Batch_acc: 0.285548. Batch_loss: 2.095926 \n",
      "Batch: 771. Acc: 0.232906. Loss: 2.241770. Batch_acc: 0.267315. Batch_loss: 2.075329 \n",
      "Batch: 772. Acc: 0.232941. Loss: 2.241593. Batch_acc: 0.260163. Batch_loss: 2.104013 \n",
      "Batch: 773. Acc: 0.233002. Loss: 2.241358. Batch_acc: 0.278900. Batch_loss: 2.064019 \n",
      "Batch: 774. Acc: 0.233059. Loss: 2.241156. Batch_acc: 0.277233. Batch_loss: 2.084634 \n",
      "Batch: 775. Acc: 0.233127. Loss: 2.240926. Batch_acc: 0.284594. Batch_loss: 2.067344 \n",
      "Batch: 776. Acc: 0.233183. Loss: 2.240739. Batch_acc: 0.277011. Batch_loss: 2.095714 \n",
      "Batch: 777. Acc: 0.233228. Loss: 2.240594. Batch_acc: 0.269095. Batch_loss: 2.125741 \n",
      "Batch: 778. Acc: 0.233281. Loss: 2.240421. Batch_acc: 0.274693. Batch_loss: 2.103726 \n",
      "Batch: 779. Acc: 0.233339. Loss: 2.240233. Batch_acc: 0.277904. Batch_loss: 2.095347 \n",
      "Batch: 780. Acc: 0.233389. Loss: 2.240070. Batch_acc: 0.272517. Batch_loss: 2.111819 \n",
      "Batch: 781. Acc: 0.233435. Loss: 2.239921. Batch_acc: 0.269143. Batch_loss: 2.124613 \n",
      "Batch: 782. Acc: 0.233487. Loss: 2.239733. Batch_acc: 0.274840. Batch_loss: 2.091404 \n",
      "Batch: 783. Acc: 0.233546. Loss: 2.239523. Batch_acc: 0.279070. Batch_loss: 2.077482 \n",
      "Batch: 784. Acc: 0.233601. Loss: 2.239332. Batch_acc: 0.276744. Batch_loss: 2.087622 \n",
      "Batch: 785. Acc: 0.233652. Loss: 2.239145. Batch_acc: 0.273190. Batch_loss: 2.094929 \n",
      "Batch: 786. Acc: 0.233705. Loss: 2.238941. Batch_acc: 0.275645. Batch_loss: 2.079525 \n",
      "Batch: 787. Acc: 0.233750. Loss: 2.238753. Batch_acc: 0.270190. Batch_loss: 2.085720 \n",
      "Batch: 788. Acc: 0.233790. Loss: 2.238590. Batch_acc: 0.265815. Batch_loss: 2.108976 \n",
      "Batch: 789. Acc: 0.233848. Loss: 2.238399. Batch_acc: 0.278345. Batch_loss: 2.090437 \n",
      "Batch: 790. Acc: 0.233909. Loss: 2.238220. Batch_acc: 0.282360. Batch_loss: 2.097092 \n",
      "Batch: 791. Acc: 0.233938. Loss: 2.238064. Batch_acc: 0.257143. Batch_loss: 2.112791 \n",
      "Batch: 792. Acc: 0.233976. Loss: 2.237878. Batch_acc: 0.263800. Batch_loss: 2.089011 \n",
      "Batch: 793. Acc: 0.234032. Loss: 2.237695. Batch_acc: 0.278990. Batch_loss: 2.093185 \n",
      "Batch: 794. Acc: 0.234085. Loss: 2.237506. Batch_acc: 0.275626. Batch_loss: 2.089281 \n",
      "Batch: 795. Acc: 0.234128. Loss: 2.237333. Batch_acc: 0.268012. Batch_loss: 2.099155 \n",
      "Batch: 796. Acc: 0.234196. Loss: 2.237121. Batch_acc: 0.288495. Batch_loss: 2.069562 \n",
      "Batch: 797. Acc: 0.234241. Loss: 2.236957. Batch_acc: 0.270006. Batch_loss: 2.106242 \n",
      "Batch: 798. Acc: 0.234286. Loss: 2.236790. Batch_acc: 0.270870. Batch_loss: 2.100842 \n",
      "Batch: 799. Acc: 0.234339. Loss: 2.236612. Batch_acc: 0.275921. Batch_loss: 2.097085 \n",
      "Batch: 800. Acc: 0.234394. Loss: 2.236422. Batch_acc: 0.277841. Batch_loss: 2.086378 \n",
      "Batch: 801. Acc: 0.234466. Loss: 2.236226. Batch_acc: 0.291011. Batch_loss: 2.083045 \n",
      "Batch: 802. Acc: 0.234533. Loss: 2.236011. Batch_acc: 0.287337. Batch_loss: 2.065422 \n",
      "Batch: 803. Acc: 0.234573. Loss: 2.235825. Batch_acc: 0.265837. Batch_loss: 2.089007 \n",
      "Batch: 804. Acc: 0.234627. Loss: 2.235624. Batch_acc: 0.279043. Batch_loss: 2.071821 \n",
      "Batch: 805. Acc: 0.234673. Loss: 2.235462. Batch_acc: 0.272673. Batch_loss: 2.101147 \n",
      "Batch: 806. Acc: 0.234726. Loss: 2.235298. Batch_acc: 0.277520. Batch_loss: 2.102368 \n",
      "Batch: 807. Acc: 0.234780. Loss: 2.235111. Batch_acc: 0.277001. Batch_loss: 2.087985 \n",
      "Batch: 808. Acc: 0.234832. Loss: 2.234952. Batch_acc: 0.277520. Batch_loss: 2.105726 \n",
      "Batch: 809. Acc: 0.234894. Loss: 2.234736. Batch_acc: 0.285714. Batch_loss: 2.058554 \n",
      "Batch: 810. Acc: 0.234945. Loss: 2.234579. Batch_acc: 0.276508. Batch_loss: 2.105543 \n",
      "Batch: 811. Acc: 0.235007. Loss: 2.234383. Batch_acc: 0.283731. Batch_loss: 2.080640 \n",
      "Batch: 812. Acc: 0.235070. Loss: 2.234171. Batch_acc: 0.285551. Batch_loss: 2.063718 \n",
      "Batch: 813. Acc: 0.235123. Loss: 2.234001. Batch_acc: 0.277937. Batch_loss: 2.096484 \n",
      "Batch: 814. Acc: 0.235194. Loss: 2.233770. Batch_acc: 0.292669. Batch_loss: 2.046227 \n",
      "Batch: 815. Acc: 0.235255. Loss: 2.233564. Batch_acc: 0.284576. Batch_loss: 2.067422 \n",
      "Batch: 816. Acc: 0.235302. Loss: 2.233393. Batch_acc: 0.273140. Batch_loss: 2.095927 \n",
      "Batch: 817. Acc: 0.235364. Loss: 2.233193. Batch_acc: 0.285959. Batch_loss: 2.070610 \n",
      "Batch: 818. Acc: 0.235454. Loss: 2.232965. Batch_acc: 0.307475. Batch_loss: 2.049544 \n",
      "Batch: 819. Acc: 0.235513. Loss: 2.232797. Batch_acc: 0.283838. Batch_loss: 2.096011 \n",
      "Batch: 820. Acc: 0.235575. Loss: 2.232604. Batch_acc: 0.287123. Batch_loss: 2.072997 \n",
      "Batch: 821. Acc: 0.235598. Loss: 2.232487. Batch_acc: 0.254662. Batch_loss: 2.135651 \n",
      "Batch: 822. Acc: 0.235646. Loss: 2.232324. Batch_acc: 0.275742. Batch_loss: 2.096449 \n",
      "Batch: 823. Acc: 0.235699. Loss: 2.232178. Batch_acc: 0.279765. Batch_loss: 2.109744 \n",
      "Batch: 824. Acc: 0.235768. Loss: 2.231946. Batch_acc: 0.289967. Batch_loss: 2.048747 \n",
      "Batch: 825. Acc: 0.235807. Loss: 2.231787. Batch_acc: 0.268519. Batch_loss: 2.099988 \n",
      "Batch: 826. Acc: 0.235851. Loss: 2.231644. Batch_acc: 0.271745. Batch_loss: 2.115190 \n",
      "Batch: 827. Acc: 0.235896. Loss: 2.231472. Batch_acc: 0.272520. Batch_loss: 2.090394 \n",
      "Batch: 828. Acc: 0.235918. Loss: 2.231342. Batch_acc: 0.254181. Batch_loss: 2.126592 \n",
      "Batch: 829. Acc: 0.235967. Loss: 2.231160. Batch_acc: 0.275626. Batch_loss: 2.081916 \n",
      "Batch: 830. Acc: 0.236003. Loss: 2.230948. Batch_acc: 0.265135. Batch_loss: 2.059378 \n",
      "Batch: 831. Acc: 0.236060. Loss: 2.230751. Batch_acc: 0.282572. Batch_loss: 2.070793 \n",
      "Batch: 832. Acc: 0.236101. Loss: 2.230616. Batch_acc: 0.270023. Batch_loss: 2.118515 \n",
      "Batch: 833. Acc: 0.236136. Loss: 2.230484. Batch_acc: 0.266990. Batch_loss: 2.114083 \n",
      "Batch: 834. Acc: 0.236199. Loss: 2.230287. Batch_acc: 0.288417. Batch_loss: 2.067070 \n",
      "Batch: 835. Acc: 0.236241. Loss: 2.230114. Batch_acc: 0.271889. Batch_loss: 2.085359 \n",
      "Batch: 836. Acc: 0.236296. Loss: 2.229932. Batch_acc: 0.281321. Batch_loss: 2.078890 \n",
      "Batch: 837. Acc: 0.236335. Loss: 2.229758. Batch_acc: 0.269430. Batch_loss: 2.084680 \n",
      "Batch: 838. Acc: 0.236391. Loss: 2.229572. Batch_acc: 0.282066. Batch_loss: 2.075275 \n",
      "Batch: 839. Acc: 0.236449. Loss: 2.229394. Batch_acc: 0.284897. Batch_loss: 2.081150 \n",
      "Batch: 840. Acc: 0.236512. Loss: 2.229206. Batch_acc: 0.290793. Batch_loss: 2.068825 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 841. Acc: 0.236569. Loss: 2.229033. Batch_acc: 0.284642. Batch_loss: 2.083010 \n",
      "Batch: 842. Acc: 0.236603. Loss: 2.228891. Batch_acc: 0.265130. Batch_loss: 2.108960 \n",
      "Batch: 843. Acc: 0.236641. Loss: 2.228745. Batch_acc: 0.268986. Batch_loss: 2.104736 \n",
      "Batch: 844. Acc: 0.236678. Loss: 2.228580. Batch_acc: 0.267468. Batch_loss: 2.090385 \n",
      "Batch: 845. Acc: 0.236719. Loss: 2.228419. Batch_acc: 0.271828. Batch_loss: 2.090645 \n",
      "Batch: 846. Acc: 0.236778. Loss: 2.228215. Batch_acc: 0.287623. Batch_loss: 2.053750 \n",
      "Batch: 847. Acc: 0.236817. Loss: 2.228077. Batch_acc: 0.268823. Batch_loss: 2.114770 \n",
      "Batch: 848. Acc: 0.236868. Loss: 2.227911. Batch_acc: 0.278819. Batch_loss: 2.089081 \n",
      "Batch: 849. Acc: 0.236912. Loss: 2.227750. Batch_acc: 0.275540. Batch_loss: 2.088746 \n",
      "Batch: 850. Acc: 0.236958. Loss: 2.227579. Batch_acc: 0.275723. Batch_loss: 2.081585 \n",
      "Batch: 851. Acc: 0.236992. Loss: 2.227423. Batch_acc: 0.266944. Batch_loss: 2.090703 \n",
      "Batch: 852. Acc: 0.237040. Loss: 2.227263. Batch_acc: 0.277556. Batch_loss: 2.091376 \n",
      "Batch: 853. Acc: 0.237059. Loss: 2.227194. Batch_acc: 0.253488. Batch_loss: 2.168164 \n",
      "Batch: 854. Acc: 0.237095. Loss: 2.227028. Batch_acc: 0.268717. Batch_loss: 2.083324 \n",
      "Batch: 855. Acc: 0.237155. Loss: 2.226839. Batch_acc: 0.287089. Batch_loss: 2.067852 \n",
      "Batch: 856. Acc: 0.237191. Loss: 2.226715. Batch_acc: 0.269117. Batch_loss: 2.117452 \n",
      "Batch: 857. Acc: 0.237250. Loss: 2.226535. Batch_acc: 0.288106. Batch_loss: 2.071749 \n",
      "Batch: 858. Acc: 0.237304. Loss: 2.226366. Batch_acc: 0.282254. Batch_loss: 2.084892 \n",
      "Batch: 859. Acc: 0.237349. Loss: 2.226214. Batch_acc: 0.276437. Batch_loss: 2.095403 \n",
      "Batch: 860. Acc: 0.237391. Loss: 2.226037. Batch_acc: 0.273830. Batch_loss: 2.072949 \n",
      "Batch: 861. Acc: 0.237467. Loss: 2.225828. Batch_acc: 0.302540. Batch_loss: 2.045139 \n",
      "Batch: 862. Acc: 0.237524. Loss: 2.225620. Batch_acc: 0.285714. Batch_loss: 2.050799 \n",
      "Batch: 863. Acc: 0.237559. Loss: 2.225463. Batch_acc: 0.267566. Batch_loss: 2.092475 \n",
      "Batch: 864. Acc: 0.237601. Loss: 2.225294. Batch_acc: 0.272676. Batch_loss: 2.081481 \n",
      "Batch: 865. Acc: 0.237650. Loss: 2.225101. Batch_acc: 0.278725. Batch_loss: 2.066200 \n",
      "Batch: 866. Acc: 0.237688. Loss: 2.224929. Batch_acc: 0.270115. Batch_loss: 2.075887 \n",
      "Batch: 867. Acc: 0.237742. Loss: 2.224756. Batch_acc: 0.284404. Batch_loss: 2.075002 \n",
      "Batch: 868. Acc: 0.237765. Loss: 2.224638. Batch_acc: 0.258480. Batch_loss: 2.120617 \n",
      "Batch: 869. Acc: 0.237811. Loss: 2.224486. Batch_acc: 0.277907. Batch_loss: 2.091235 \n",
      "Batch: 870. Acc: 0.237843. Loss: 2.224345. Batch_acc: 0.265651. Batch_loss: 2.103982 \n",
      "Batch: 871. Acc: 0.237903. Loss: 2.224179. Batch_acc: 0.289040. Batch_loss: 2.081445 \n",
      "Batch: 872. Acc: 0.237964. Loss: 2.223990. Batch_acc: 0.290525. Batch_loss: 2.060431 \n",
      "Batch: 873. Acc: 0.238011. Loss: 2.223839. Batch_acc: 0.278698. Batch_loss: 2.092651 \n",
      "Batch: 874. Acc: 0.238066. Loss: 2.223646. Batch_acc: 0.286449. Batch_loss: 2.056146 \n",
      "Batch: 875. Acc: 0.238104. Loss: 2.223516. Batch_acc: 0.271771. Batch_loss: 2.107834 \n",
      "Batch: 876. Acc: 0.238160. Loss: 2.223359. Batch_acc: 0.288066. Batch_loss: 2.082833 \n",
      "Batch: 877. Acc: 0.238230. Loss: 2.223165. Batch_acc: 0.300409. Batch_loss: 2.050316 \n",
      "Batch: 878. Acc: 0.238271. Loss: 2.222989. Batch_acc: 0.274019. Batch_loss: 2.069998 \n",
      "Batch: 879. Acc: 0.238314. Loss: 2.222808. Batch_acc: 0.274652. Batch_loss: 2.068687 \n",
      "Batch: 880. Acc: 0.238373. Loss: 2.222600. Batch_acc: 0.288988. Batch_loss: 2.045105 \n",
      "Batch: 881. Acc: 0.238418. Loss: 2.222460. Batch_acc: 0.279300. Batch_loss: 2.097272 \n",
      "Batch: 882. Acc: 0.238471. Loss: 2.222300. Batch_acc: 0.285131. Batch_loss: 2.078994 \n",
      "Batch: 883. Acc: 0.238521. Loss: 2.222167. Batch_acc: 0.282921. Batch_loss: 2.104456 \n",
      "Batch: 884. Acc: 0.238561. Loss: 2.222031. Batch_acc: 0.274408. Batch_loss: 2.101557 \n",
      "Batch: 885. Acc: 0.238596. Loss: 2.221882. Batch_acc: 0.269656. Batch_loss: 2.088112 \n",
      "Batch: 886. Acc: 0.238628. Loss: 2.221769. Batch_acc: 0.267763. Batch_loss: 2.119490 \n",
      "Batch: 887. Acc: 0.238668. Loss: 2.221604. Batch_acc: 0.273140. Batch_loss: 2.077380 \n",
      "Batch: 888. Acc: 0.238701. Loss: 2.221465. Batch_acc: 0.268418. Batch_loss: 2.099394 \n",
      "Batch: 889. Acc: 0.238749. Loss: 2.221310. Batch_acc: 0.280662. Batch_loss: 2.084652 \n",
      "Batch: 890. Acc: 0.238799. Loss: 2.221136. Batch_acc: 0.282782. Batch_loss: 2.067570 \n",
      "Batch: 891. Acc: 0.238839. Loss: 2.220939. Batch_acc: 0.274770. Batch_loss: 2.044785 \n",
      "Batch: 892. Acc: 0.238875. Loss: 2.220800. Batch_acc: 0.270797. Batch_loss: 2.097446 \n",
      "Batch: 893. Acc: 0.238919. Loss: 2.220647. Batch_acc: 0.278576. Batch_loss: 2.084164 \n",
      "Batch: 894. Acc: 0.238970. Loss: 2.220497. Batch_acc: 0.282998. Batch_loss: 2.089880 \n",
      "Batch: 895. Acc: 0.239004. Loss: 2.220395. Batch_acc: 0.269454. Batch_loss: 2.128155 \n",
      "Batch: 896. Acc: 0.239048. Loss: 2.220244. Batch_acc: 0.279017. Batch_loss: 2.085539 \n",
      "Batch: 897. Acc: 0.239094. Loss: 2.220070. Batch_acc: 0.280958. Batch_loss: 2.061980 \n",
      "Batch: 898. Acc: 0.239132. Loss: 2.219945. Batch_acc: 0.272934. Batch_loss: 2.108650 \n",
      "Batch: 899. Acc: 0.239178. Loss: 2.219780. Batch_acc: 0.279704. Batch_loss: 2.073469 \n",
      "Batch: 900. Acc: 0.239208. Loss: 2.219634. Batch_acc: 0.265945. Batch_loss: 2.089163 \n",
      "Batch: 901. Acc: 0.239258. Loss: 2.219451. Batch_acc: 0.284078. Batch_loss: 2.054887 \n",
      "Batch: 902. Acc: 0.239310. Loss: 2.219264. Batch_acc: 0.285393. Batch_loss: 2.055136 \n",
      "Batch: 903. Acc: 0.239351. Loss: 2.219131. Batch_acc: 0.276833. Batch_loss: 2.096492 \n",
      "Batch: 904. Acc: 0.239346. Loss: 2.219042. Batch_acc: 0.234634. Batch_loss: 2.136574 \n",
      "Batch: 905. Acc: 0.239376. Loss: 2.218910. Batch_acc: 0.266171. Batch_loss: 2.099847 \n",
      "Batch: 906. Acc: 0.239420. Loss: 2.218775. Batch_acc: 0.280162. Batch_loss: 2.094820 \n",
      "Batch: 907. Acc: 0.239467. Loss: 2.218643. Batch_acc: 0.283196. Batch_loss: 2.096491 \n",
      "Batch: 908. Acc: 0.239527. Loss: 2.218444. Batch_acc: 0.292711. Batch_loss: 2.039594 \n",
      "Batch: 909. Acc: 0.239569. Loss: 2.218304. Batch_acc: 0.278496. Batch_loss: 2.088212 \n",
      "Batch: 910. Acc: 0.239593. Loss: 2.218173. Batch_acc: 0.261089. Batch_loss: 2.101794 \n",
      "Batch: 911. Acc: 0.239627. Loss: 2.218044. Batch_acc: 0.269726. Batch_loss: 2.104326 \n",
      "Batch: 912. Acc: 0.239658. Loss: 2.217900. Batch_acc: 0.268805. Batch_loss: 2.084259 \n",
      "Batch: 913. Acc: 0.239711. Loss: 2.217730. Batch_acc: 0.288097. Batch_loss: 2.062316 \n",
      "Batch: 914. Acc: 0.239766. Loss: 2.217534. Batch_acc: 0.288183. Batch_loss: 2.043996 \n",
      "Batch: 915. Acc: 0.239811. Loss: 2.217369. Batch_acc: 0.281034. Batch_loss: 2.066403 \n",
      "Batch: 916. Acc: 0.239849. Loss: 2.217249. Batch_acc: 0.275304. Batch_loss: 2.107015 \n",
      "Batch: 917. Acc: 0.239889. Loss: 2.217108. Batch_acc: 0.277450. Batch_loss: 2.084297 \n",
      "Batch: 918. Acc: 0.239923. Loss: 2.216987. Batch_acc: 0.270056. Batch_loss: 2.108262 \n",
      "Batch: 919. Acc: 0.239954. Loss: 2.216850. Batch_acc: 0.268376. Batch_loss: 2.092235 \n",
      "Batch: 920. Acc: 0.240010. Loss: 2.216671. Batch_acc: 0.291096. Batch_loss: 2.053162 \n",
      "Batch: 921. Acc: 0.240060. Loss: 2.216494. Batch_acc: 0.284992. Batch_loss: 2.056541 \n",
      "Batch: 922. Acc: 0.240095. Loss: 2.216359. Batch_acc: 0.272834. Batch_loss: 2.089002 \n",
      "Batch: 923. Acc: 0.240135. Loss: 2.216226. Batch_acc: 0.278592. Batch_loss: 2.090946 \n",
      "Batch: 924. Acc: 0.240186. Loss: 2.216074. Batch_acc: 0.287544. Batch_loss: 2.074246 \n",
      "Batch: 925. Acc: 0.240230. Loss: 2.215910. Batch_acc: 0.280692. Batch_loss: 2.063504 \n",
      "Batch: 926. Acc: 0.240258. Loss: 2.215760. Batch_acc: 0.266589. Batch_loss: 2.075946 \n",
      "Batch: 927. Acc: 0.240304. Loss: 2.215590. Batch_acc: 0.282932. Batch_loss: 2.058701 \n",
      "Batch: 928. Acc: 0.240334. Loss: 2.215451. Batch_acc: 0.267685. Batch_loss: 2.087835 \n",
      "Batch: 929. Acc: 0.240371. Loss: 2.215318. Batch_acc: 0.274668. Batch_loss: 2.091263 \n",
      "Batch: 930. Acc: 0.240399. Loss: 2.215186. Batch_acc: 0.267943. Batch_loss: 2.088125 \n",
      "Batch: 931. Acc: 0.240434. Loss: 2.215032. Batch_acc: 0.272314. Batch_loss: 2.073374 \n",
      "Batch: 932. Acc: 0.240465. Loss: 2.214914. Batch_acc: 0.269841. Batch_loss: 2.102627 \n",
      "Batch: 933. Acc: 0.240510. Loss: 2.214786. Batch_acc: 0.283768. Batch_loss: 2.091599 \n",
      "Batch: 934. Acc: 0.240552. Loss: 2.214619. Batch_acc: 0.280207. Batch_loss: 2.058169 \n",
      "Batch: 935. Acc: 0.240593. Loss: 2.214461. Batch_acc: 0.278830. Batch_loss: 2.067521 \n",
      "Batch: 936. Acc: 0.240633. Loss: 2.214325. Batch_acc: 0.278422. Batch_loss: 2.085916 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 937. Acc: 0.240685. Loss: 2.214166. Batch_acc: 0.288059. Batch_loss: 2.067142 \n",
      "Batch: 938. Acc: 0.240715. Loss: 2.214031. Batch_acc: 0.268732. Batch_loss: 2.089836 \n",
      "Batch: 939. Acc: 0.240747. Loss: 2.213878. Batch_acc: 0.270715. Batch_loss: 2.072808 \n",
      "Batch: 940. Acc: 0.240795. Loss: 2.213721. Batch_acc: 0.285880. Batch_loss: 2.064743 \n",
      "Batch: 941. Acc: 0.240843. Loss: 2.213575. Batch_acc: 0.286290. Batch_loss: 2.075513 \n",
      "Batch: 942. Acc: 0.240865. Loss: 2.213453. Batch_acc: 0.261822. Batch_loss: 2.098753 \n",
      "Batch: 943. Acc: 0.240905. Loss: 2.213299. Batch_acc: 0.277716. Batch_loss: 2.071386 \n",
      "Batch: 944. Acc: 0.240932. Loss: 2.213173. Batch_acc: 0.266007. Batch_loss: 2.092701 \n",
      "Batch: 945. Acc: 0.240970. Loss: 2.213036. Batch_acc: 0.276826. Batch_loss: 2.084394 \n",
      "Batch: 946. Acc: 0.241025. Loss: 2.212846. Batch_acc: 0.292325. Batch_loss: 2.036957 \n",
      "Batch: 947. Acc: 0.241080. Loss: 2.212672. Batch_acc: 0.293811. Batch_loss: 2.047090 \n",
      "Batch: 948. Acc: 0.241133. Loss: 2.212516. Batch_acc: 0.290452. Batch_loss: 2.065583 \n",
      "Batch: 949. Acc: 0.241173. Loss: 2.212370. Batch_acc: 0.279284. Batch_loss: 2.072984 \n",
      "Batch: 950. Acc: 0.241198. Loss: 2.212246. Batch_acc: 0.264908. Batch_loss: 2.094499 \n",
      "Batch: 951. Acc: 0.241232. Loss: 2.212107. Batch_acc: 0.273789. Batch_loss: 2.078275 \n",
      "Batch: 952. Acc: 0.241277. Loss: 2.211931. Batch_acc: 0.285134. Batch_loss: 2.043181 \n",
      "Batch: 953. Acc: 0.241301. Loss: 2.211822. Batch_acc: 0.264988. Batch_loss: 2.103339 \n",
      "Batch: 954. Acc: 0.241344. Loss: 2.211688. Batch_acc: 0.282421. Batch_loss: 2.083855 \n",
      "Batch: 955. Acc: 0.241387. Loss: 2.211566. Batch_acc: 0.281948. Batch_loss: 2.094604 \n",
      "Batch: 956. Acc: 0.241426. Loss: 2.211425. Batch_acc: 0.279300. Batch_loss: 2.075494 \n",
      "Batch: 957. Acc: 0.241473. Loss: 2.211311. Batch_acc: 0.286701. Batch_loss: 2.102030 \n",
      "Batch: 958. Acc: 0.241517. Loss: 2.211133. Batch_acc: 0.282755. Batch_loss: 2.044905 \n",
      "Batch: 959. Acc: 0.241551. Loss: 2.210995. Batch_acc: 0.273349. Batch_loss: 2.080066 \n",
      "Batch: 960. Acc: 0.241608. Loss: 2.210850. Batch_acc: 0.296848. Batch_loss: 2.071972 \n",
      "Batch: 961. Acc: 0.241644. Loss: 2.210730. Batch_acc: 0.274565. Batch_loss: 2.098301 \n",
      "Batch: 962. Acc: 0.241691. Loss: 2.210589. Batch_acc: 0.287766. Batch_loss: 2.075154 \n",
      "Batch: 963. Acc: 0.241721. Loss: 2.210492. Batch_acc: 0.271217. Batch_loss: 2.113887 \n",
      "Batch: 964. Acc: 0.241761. Loss: 2.210330. Batch_acc: 0.279613. Batch_loss: 2.056246 \n",
      "Batch: 965. Acc: 0.241799. Loss: 2.210191. Batch_acc: 0.278698. Batch_loss: 2.076975 \n",
      "Batch: 966. Acc: 0.241846. Loss: 2.210058. Batch_acc: 0.287719. Batch_loss: 2.078776 \n",
      "Batch: 967. Acc: 0.241877. Loss: 2.209929. Batch_acc: 0.272460. Batch_loss: 2.083211 \n",
      "Batch: 968. Acc: 0.241930. Loss: 2.209765. Batch_acc: 0.292559. Batch_loss: 2.053767 \n",
      "Batch: 969. Acc: 0.241976. Loss: 2.209617. Batch_acc: 0.285878. Batch_loss: 2.067028 \n",
      "Batch: 970. Acc: 0.242015. Loss: 2.209480. Batch_acc: 0.280047. Batch_loss: 2.074590 \n",
      "Batch: 971. Acc: 0.242046. Loss: 2.209358. Batch_acc: 0.272781. Batch_loss: 2.088888 \n",
      "Batch: 972. Acc: 0.242076. Loss: 2.209246. Batch_acc: 0.272834. Batch_loss: 2.097550 \n",
      "Batch: 973. Acc: 0.242111. Loss: 2.209089. Batch_acc: 0.275961. Batch_loss: 2.056229 \n",
      "Batch: 974. Acc: 0.242163. Loss: 2.208962. Batch_acc: 0.292584. Batch_loss: 2.084745 \n",
      "Batch: 975. Acc: 0.242204. Loss: 2.208791. Batch_acc: 0.282139. Batch_loss: 2.043786 \n",
      "Batch: 976. Acc: 0.242248. Loss: 2.208670. Batch_acc: 0.285632. Batch_loss: 2.089656 \n",
      "Batch: 977. Acc: 0.242284. Loss: 2.208533. Batch_acc: 0.277199. Batch_loss: 2.074147 \n",
      "Batch: 978. Acc: 0.242325. Loss: 2.208396. Batch_acc: 0.282125. Batch_loss: 2.074703 \n",
      "Batch: 979. Acc: 0.242357. Loss: 2.208271. Batch_acc: 0.274109. Batch_loss: 2.084456 \n",
      "Batch: 980. Acc: 0.242388. Loss: 2.208145. Batch_acc: 0.273406. Batch_loss: 2.084413 \n",
      "Batch: 981. Acc: 0.242418. Loss: 2.208049. Batch_acc: 0.271925. Batch_loss: 2.112156 \n",
      "Batch: 982. Acc: 0.242451. Loss: 2.207921. Batch_acc: 0.275329. Batch_loss: 2.082380 \n",
      "Batch: 983. Acc: 0.242498. Loss: 2.207757. Batch_acc: 0.287422. Batch_loss: 2.048380 \n",
      "Batch: 984. Acc: 0.242530. Loss: 2.207619. Batch_acc: 0.274194. Batch_loss: 2.071645 \n",
      "Batch: 985. Acc: 0.242566. Loss: 2.207471. Batch_acc: 0.277715. Batch_loss: 2.064275 \n",
      "Batch: 986. Acc: 0.242607. Loss: 2.207355. Batch_acc: 0.282696. Batch_loss: 2.093422 \n",
      "Batch: 987. Acc: 0.242644. Loss: 2.207223. Batch_acc: 0.278846. Batch_loss: 2.079948 \n",
      "Batch: 988. Acc: 0.242681. Loss: 2.207087. Batch_acc: 0.278416. Batch_loss: 2.072969 \n",
      "Batch: 989. Acc: 0.242721. Loss: 2.206949. Batch_acc: 0.284279. Batch_loss: 2.066341 \n",
      "Batch: 990. Acc: 0.242761. Loss: 2.206836. Batch_acc: 0.282367. Batch_loss: 2.093045 \n",
      "Batch: 991. Acc: 0.242789. Loss: 2.206706. Batch_acc: 0.271765. Batch_loss: 2.075101 \n",
      "Batch: 992. Acc: 0.242830. Loss: 2.206556. Batch_acc: 0.283019. Batch_loss: 2.058430 \n",
      "Batch: 993. Acc: 0.242883. Loss: 2.206403. Batch_acc: 0.295390. Batch_loss: 2.055795 \n",
      "Batch: 994. Acc: 0.242922. Loss: 2.206268. Batch_acc: 0.281537. Batch_loss: 2.073046 \n",
      "Batch: 995. Acc: 0.242955. Loss: 2.206137. Batch_acc: 0.275198. Batch_loss: 2.077699 \n",
      "Batch: 996. Acc: 0.242989. Loss: 2.206011. Batch_acc: 0.276941. Batch_loss: 2.079605 \n",
      "Batch: 997. Acc: 0.243045. Loss: 2.205872. Batch_acc: 0.297994. Batch_loss: 2.067156 \n",
      "Batch: 998. Acc: 0.243094. Loss: 2.205715. Batch_acc: 0.292404. Batch_loss: 2.050446 \n",
      "Batch: 999. Acc: 0.243129. Loss: 2.205597. Batch_acc: 0.278200. Batch_loss: 2.086178 \n",
      "Batch: 1000. Acc: 0.243167. Loss: 2.205467. Batch_acc: 0.281682. Batch_loss: 2.075305 \n",
      "Batch: 1001. Acc: 0.243216. Loss: 2.205322. Batch_acc: 0.291954. Batch_loss: 2.059946 \n",
      "Batch: 1002. Acc: 0.243254. Loss: 2.205160. Batch_acc: 0.281413. Batch_loss: 2.042142 \n",
      "Batch: 1003. Acc: 0.243285. Loss: 2.205044. Batch_acc: 0.274578. Batch_loss: 2.087251 \n",
      "Batch: 1004. Acc: 0.243321. Loss: 2.204940. Batch_acc: 0.280325. Batch_loss: 2.099110 \n",
      "Checkpointing on batch: 1004. Accuracy: 0.24332125459391796. Loss per char: 2.2049397417506764. Time: 1627203358.5797987\n",
      "Last question is tensor([ 2, 20, 18, 22, 25, 19, 12, 18, 17, 21, 21, 15, 20, 25, 26, 24,  3,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0], device='cuda:0')\n",
      "Removing existing model file at checkpoints\\add_or_sub-2021-07-25_latest_checkpoint.pth\n",
      "Starting checkpoint save of checkpoints\\add_or_sub-2021-07-25_latest_checkpoint.pth...\n",
      "Final saved model size: 530790651\n",
      "Batch: 1005. Acc: 0.243372. Loss: 2.204795. Batch_acc: 0.293817. Batch_loss: 2.061704 \n",
      "Batch: 1006. Acc: 0.243414. Loss: 2.204639. Batch_acc: 0.284576. Batch_loss: 2.048552 \n",
      "Batch: 1007. Acc: 0.243444. Loss: 2.204528. Batch_acc: 0.274764. Batch_loss: 2.090255 \n",
      "Batch: 1008. Acc: 0.243469. Loss: 2.204409. Batch_acc: 0.268545. Batch_loss: 2.084838 \n",
      "Batch: 1009. Acc: 0.243504. Loss: 2.204292. Batch_acc: 0.279232. Batch_loss: 2.084546 \n",
      "Batch: 1010. Acc: 0.243554. Loss: 2.204149. Batch_acc: 0.293682. Batch_loss: 2.061020 \n",
      "Batch: 1011. Acc: 0.243589. Loss: 2.204031. Batch_acc: 0.279343. Batch_loss: 2.082812 \n",
      "Batch: 1012. Acc: 0.243632. Loss: 2.203893. Batch_acc: 0.286937. Batch_loss: 2.065397 \n",
      "Batch: 1013. Acc: 0.243682. Loss: 2.203736. Batch_acc: 0.295799. Batch_loss: 2.041868 \n",
      "Batch: 1014. Acc: 0.243703. Loss: 2.203641. Batch_acc: 0.264434. Batch_loss: 2.107630 \n",
      "Batch: 1015. Acc: 0.243731. Loss: 2.203529. Batch_acc: 0.272833. Batch_loss: 2.088042 \n",
      "Batch: 1016. Acc: 0.243773. Loss: 2.203384. Batch_acc: 0.285237. Batch_loss: 2.061487 \n",
      "Batch: 1017. Acc: 0.243791. Loss: 2.203278. Batch_acc: 0.262041. Batch_loss: 2.095107 \n",
      "Batch: 1018. Acc: 0.243816. Loss: 2.203169. Batch_acc: 0.269611. Batch_loss: 2.091419 \n",
      "Batch: 1019. Acc: 0.243850. Loss: 2.203057. Batch_acc: 0.278727. Batch_loss: 2.085901 \n",
      "Batch: 1020. Acc: 0.243881. Loss: 2.202950. Batch_acc: 0.275761. Batch_loss: 2.092447 \n",
      "Batch: 1021. Acc: 0.243907. Loss: 2.202814. Batch_acc: 0.270953. Batch_loss: 2.063905 \n",
      "Batch: 1022. Acc: 0.243937. Loss: 2.202693. Batch_acc: 0.275029. Batch_loss: 2.078917 \n",
      "Batch: 1023. Acc: 0.243976. Loss: 2.202543. Batch_acc: 0.282547. Batch_loss: 2.051124 \n",
      "Batch: 1024. Acc: 0.244006. Loss: 2.202410. Batch_acc: 0.274103. Batch_loss: 2.069691 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 1025. Acc: 0.244046. Loss: 2.202260. Batch_acc: 0.284746. Batch_loss: 2.050782 \n",
      "Batch: 1026. Acc: 0.244091. Loss: 2.202127. Batch_acc: 0.290780. Batch_loss: 2.061925 \n",
      "Batch: 1027. Acc: 0.244126. Loss: 2.201990. Batch_acc: 0.280551. Batch_loss: 2.061507 \n",
      "Batch: 1028. Acc: 0.244178. Loss: 2.201853. Batch_acc: 0.297564. Batch_loss: 2.059909 \n",
      "Batch: 1029. Acc: 0.244192. Loss: 2.201769. Batch_acc: 0.259281. Batch_loss: 2.113453 \n",
      "Batch: 1030. Acc: 0.244227. Loss: 2.201637. Batch_acc: 0.280692. Batch_loss: 2.065763 \n",
      "Batch: 1031. Acc: 0.244257. Loss: 2.201533. Batch_acc: 0.275132. Batch_loss: 2.091914 \n",
      "Batch: 1032. Acc: 0.244298. Loss: 2.201415. Batch_acc: 0.288427. Batch_loss: 2.075570 \n",
      "Batch: 1033. Acc: 0.244352. Loss: 2.201295. Batch_acc: 0.300756. Batch_loss: 2.075631 \n",
      "Batch: 1034. Acc: 0.244389. Loss: 2.201147. Batch_acc: 0.282286. Batch_loss: 2.049043 \n",
      "Batch: 1035. Acc: 0.244433. Loss: 2.201017. Batch_acc: 0.292088. Batch_loss: 2.062116 \n",
      "Batch: 1036. Acc: 0.244467. Loss: 2.200913. Batch_acc: 0.279651. Batch_loss: 2.091958 \n",
      "Batch: 1037. Acc: 0.244494. Loss: 2.200821. Batch_acc: 0.272781. Batch_loss: 2.104055 \n",
      "Batch: 1038. Acc: 0.244520. Loss: 2.200701. Batch_acc: 0.271986. Batch_loss: 2.073739 \n",
      "Batch: 1039. Acc: 0.244550. Loss: 2.200581. Batch_acc: 0.276571. Batch_loss: 2.073663 \n",
      "Batch: 1040. Acc: 0.244581. Loss: 2.200457. Batch_acc: 0.276852. Batch_loss: 2.071590 \n",
      "Batch: 1041. Acc: 0.244621. Loss: 2.200345. Batch_acc: 0.286382. Batch_loss: 2.082603 \n",
      "Batch: 1042. Acc: 0.244641. Loss: 2.200257. Batch_acc: 0.265866. Batch_loss: 2.108477 \n",
      "Batch: 1043. Acc: 0.244673. Loss: 2.200150. Batch_acc: 0.277434. Batch_loss: 2.091367 \n",
      "Batch: 1044. Acc: 0.244714. Loss: 2.200025. Batch_acc: 0.286844. Batch_loss: 2.071791 \n",
      "Batch: 1045. Acc: 0.244735. Loss: 2.199903. Batch_acc: 0.267180. Batch_loss: 2.068235 \n",
      "Batch: 1046. Acc: 0.244772. Loss: 2.199771. Batch_acc: 0.281651. Batch_loss: 2.065889 \n",
      "Batch: 1047. Acc: 0.244804. Loss: 2.199634. Batch_acc: 0.278481. Batch_loss: 2.057030 \n",
      "Batch: 1048. Acc: 0.244823. Loss: 2.199530. Batch_acc: 0.264893. Batch_loss: 2.089423 \n",
      "Batch: 1049. Acc: 0.244857. Loss: 2.199434. Batch_acc: 0.279932. Batch_loss: 2.099734 \n",
      "Batch: 1050. Acc: 0.244890. Loss: 2.199293. Batch_acc: 0.280396. Batch_loss: 2.049474 \n",
      "Batch: 1051. Acc: 0.244933. Loss: 2.199178. Batch_acc: 0.292088. Batch_loss: 2.073883 \n",
      "Batch: 1052. Acc: 0.244954. Loss: 2.199069. Batch_acc: 0.266974. Batch_loss: 2.084364 \n",
      "Batch: 1053. Acc: 0.244981. Loss: 2.198940. Batch_acc: 0.272883. Batch_loss: 2.064373 \n",
      "Batch: 1054. Acc: 0.244995. Loss: 2.198836. Batch_acc: 0.260290. Batch_loss: 2.088160 \n",
      "Batch: 1055. Acc: 0.245028. Loss: 2.198703. Batch_acc: 0.279310. Batch_loss: 2.058463 \n",
      "Batch: 1056. Acc: 0.245064. Loss: 2.198607. Batch_acc: 0.285024. Batch_loss: 2.091923 \n",
      "Batch: 1057. Acc: 0.245111. Loss: 2.198451. Batch_acc: 0.295167. Batch_loss: 2.033561 \n",
      "Batch: 1058. Acc: 0.245147. Loss: 2.198326. Batch_acc: 0.282910. Batch_loss: 2.066536 \n",
      "Batch: 1059. Acc: 0.245177. Loss: 2.198218. Batch_acc: 0.276524. Batch_loss: 2.085501 \n",
      "Batch: 1060. Acc: 0.245209. Loss: 2.198101. Batch_acc: 0.279150. Batch_loss: 2.074147 \n",
      "Batch: 1061. Acc: 0.245240. Loss: 2.197998. Batch_acc: 0.278481. Batch_loss: 2.089217 \n",
      "Batch: 1062. Acc: 0.245276. Loss: 2.197903. Batch_acc: 0.282373. Batch_loss: 2.097836 \n",
      "Batch: 1063. Acc: 0.245299. Loss: 2.197800. Batch_acc: 0.271318. Batch_loss: 2.083807 \n",
      "Batch: 1064. Acc: 0.245335. Loss: 2.197669. Batch_acc: 0.282869. Batch_loss: 2.060473 \n",
      "Batch: 1065. Acc: 0.245369. Loss: 2.197560. Batch_acc: 0.281321. Batch_loss: 2.082637 \n",
      "Batch: 1066. Acc: 0.245416. Loss: 2.197426. Batch_acc: 0.295742. Batch_loss: 2.054128 \n",
      "Batch: 1067. Acc: 0.245459. Loss: 2.197293. Batch_acc: 0.289871. Batch_loss: 2.059069 \n",
      "Batch: 1068. Acc: 0.245493. Loss: 2.197172. Batch_acc: 0.281963. Batch_loss: 2.068817 \n",
      "Batch: 1069. Acc: 0.245526. Loss: 2.197064. Batch_acc: 0.280207. Batch_loss: 2.082115 \n",
      "Batch: 1070. Acc: 0.245558. Loss: 2.196935. Batch_acc: 0.280325. Batch_loss: 2.058016 \n",
      "Batch: 1071. Acc: 0.245596. Loss: 2.196824. Batch_acc: 0.287048. Batch_loss: 2.075650 \n",
      "Batch: 1072. Acc: 0.245642. Loss: 2.196672. Batch_acc: 0.294385. Batch_loss: 2.035891 \n",
      "Batch: 1073. Acc: 0.245697. Loss: 2.196520. Batch_acc: 0.302809. Batch_loss: 2.037433 \n",
      "Batch: 1074. Acc: 0.245723. Loss: 2.196403. Batch_acc: 0.273956. Batch_loss: 2.067944 \n",
      "Batch: 1075. Acc: 0.245744. Loss: 2.196315. Batch_acc: 0.269643. Batch_loss: 2.098819 \n",
      "Batch: 1076. Acc: 0.245771. Loss: 2.196197. Batch_acc: 0.275219. Batch_loss: 2.067215 \n",
      "Batch: 1077. Acc: 0.245806. Loss: 2.196101. Batch_acc: 0.283776. Batch_loss: 2.090703 \n",
      "Batch: 1078. Acc: 0.245836. Loss: 2.195992. Batch_acc: 0.278351. Batch_loss: 2.078346 \n",
      "Batch: 1079. Acc: 0.245872. Loss: 2.195865. Batch_acc: 0.284742. Batch_loss: 2.061161 \n",
      "Batch: 1080. Acc: 0.245903. Loss: 2.195751. Batch_acc: 0.279070. Batch_loss: 2.071098 \n",
      "Batch: 1081. Acc: 0.245929. Loss: 2.195639. Batch_acc: 0.274100. Batch_loss: 2.073889 \n",
      "Batch: 1082. Acc: 0.245966. Loss: 2.195502. Batch_acc: 0.286211. Batch_loss: 2.046104 \n",
      "Batch: 1083. Acc: 0.245996. Loss: 2.195399. Batch_acc: 0.279420. Batch_loss: 2.082920 \n",
      "Batch: 1084. Acc: 0.246027. Loss: 2.195288. Batch_acc: 0.279420. Batch_loss: 2.073819 \n",
      "Batch: 1085. Acc: 0.246051. Loss: 2.195191. Batch_acc: 0.272937. Batch_loss: 2.089793 \n",
      "Batch: 1086. Acc: 0.246077. Loss: 2.195076. Batch_acc: 0.274476. Batch_loss: 2.068391 \n",
      "Batch: 1087. Acc: 0.246107. Loss: 2.194953. Batch_acc: 0.278736. Batch_loss: 2.061821 \n",
      "Batch: 1088. Acc: 0.246141. Loss: 2.194835. Batch_acc: 0.282609. Batch_loss: 2.067187 \n",
      "Batch: 1089. Acc: 0.246163. Loss: 2.194708. Batch_acc: 0.269540. Batch_loss: 2.056886 \n",
      "Batch: 1090. Acc: 0.246193. Loss: 2.194583. Batch_acc: 0.278438. Batch_loss: 2.060478 \n",
      "Batch: 1091. Acc: 0.246212. Loss: 2.194480. Batch_acc: 0.267092. Batch_loss: 2.081183 \n",
      "Batch: 1092. Acc: 0.246226. Loss: 2.194403. Batch_acc: 0.262192. Batch_loss: 2.110483 \n",
      "Batch: 1093. Acc: 0.246260. Loss: 2.194298. Batch_acc: 0.282731. Batch_loss: 2.081495 \n",
      "Batch: 1094. Acc: 0.246314. Loss: 2.194162. Batch_acc: 0.306832. Batch_loss: 2.041705 \n",
      "Batch: 1095. Acc: 0.246334. Loss: 2.194100. Batch_acc: 0.268578. Batch_loss: 2.125422 \n",
      "Batch: 1096. Acc: 0.246360. Loss: 2.193995. Batch_acc: 0.274757. Batch_loss: 2.079922 \n",
      "Batch: 1097. Acc: 0.246399. Loss: 2.193881. Batch_acc: 0.288825. Batch_loss: 2.069042 \n",
      "Batch: 1098. Acc: 0.246439. Loss: 2.193747. Batch_acc: 0.289937. Batch_loss: 2.047647 \n",
      "Batch: 1099. Acc: 0.246476. Loss: 2.193615. Batch_acc: 0.286862. Batch_loss: 2.049363 \n",
      "Batch: 1100. Acc: 0.246505. Loss: 2.193518. Batch_acc: 0.279481. Batch_loss: 2.083744 \n",
      "Batch: 1101. Acc: 0.246540. Loss: 2.193391. Batch_acc: 0.284091. Batch_loss: 2.055520 \n",
      "Batch: 1102. Acc: 0.246581. Loss: 2.193268. Batch_acc: 0.292398. Batch_loss: 2.055608 \n",
      "Batch: 1103. Acc: 0.246610. Loss: 2.193165. Batch_acc: 0.278221. Batch_loss: 2.080745 \n",
      "Batch: 1104. Acc: 0.246659. Loss: 2.193023. Batch_acc: 0.300910. Batch_loss: 2.037871 \n",
      "Batch: 1105. Acc: 0.246694. Loss: 2.192895. Batch_acc: 0.285223. Batch_loss: 2.052198 \n",
      "Batch: 1106. Acc: 0.246737. Loss: 2.192773. Batch_acc: 0.295537. Batch_loss: 2.051672 \n",
      "Batch: 1107. Acc: 0.246767. Loss: 2.192668. Batch_acc: 0.280905. Batch_loss: 2.075459 \n",
      "Batch: 1108. Acc: 0.246799. Loss: 2.192555. Batch_acc: 0.282953. Batch_loss: 2.064339 \n",
      "Batch: 1109. Acc: 0.246845. Loss: 2.192416. Batch_acc: 0.297126. Batch_loss: 2.039392 \n",
      "Batch: 1110. Acc: 0.246878. Loss: 2.192316. Batch_acc: 0.283979. Batch_loss: 2.080691 \n",
      "Batch: 1111. Acc: 0.246927. Loss: 2.192188. Batch_acc: 0.300795. Batch_loss: 2.051939 \n",
      "Batch: 1112. Acc: 0.246969. Loss: 2.192053. Batch_acc: 0.293642. Batch_loss: 2.041169 \n",
      "Batch: 1113. Acc: 0.246993. Loss: 2.191955. Batch_acc: 0.274074. Batch_loss: 2.083323 \n",
      "Batch: 1114. Acc: 0.247025. Loss: 2.191857. Batch_acc: 0.281918. Batch_loss: 2.082119 \n",
      "Batch: 1115. Acc: 0.247058. Loss: 2.191744. Batch_acc: 0.285883. Batch_loss: 2.062805 \n",
      "Batch: 1116. Acc: 0.247092. Loss: 2.191610. Batch_acc: 0.284734. Batch_loss: 2.043374 \n",
      "Batch: 1117. Acc: 0.247105. Loss: 2.191537. Batch_acc: 0.262004. Batch_loss: 2.106968 \n",
      "Batch: 1118. Acc: 0.247115. Loss: 2.191459. Batch_acc: 0.258353. Batch_loss: 2.101952 \n",
      "Batch: 1119. Acc: 0.247145. Loss: 2.191333. Batch_acc: 0.279704. Batch_loss: 2.050981 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 1120. Acc: 0.247177. Loss: 2.191231. Batch_acc: 0.283401. Batch_loss: 2.076801 \n",
      "Batch: 1121. Acc: 0.247200. Loss: 2.191117. Batch_acc: 0.272675. Batch_loss: 2.065115 \n",
      "Batch: 1122. Acc: 0.247239. Loss: 2.190985. Batch_acc: 0.289977. Batch_loss: 2.045282 \n",
      "Batch: 1123. Acc: 0.247277. Loss: 2.190868. Batch_acc: 0.288988. Batch_loss: 2.063715 \n",
      "Batch: 1124. Acc: 0.247305. Loss: 2.190754. Batch_acc: 0.279043. Batch_loss: 2.063913 \n",
      "Batch: 1125. Acc: 0.247341. Loss: 2.190661. Batch_acc: 0.288416. Batch_loss: 2.083296 \n",
      "Batch: 1126. Acc: 0.247372. Loss: 2.190561. Batch_acc: 0.281787. Batch_loss: 2.078097 \n",
      "Batch: 1127. Acc: 0.247414. Loss: 2.190431. Batch_acc: 0.294658. Batch_loss: 2.044258 \n",
      "Batch: 1128. Acc: 0.247446. Loss: 2.190323. Batch_acc: 0.283607. Batch_loss: 2.070449 \n",
      "Batch: 1129. Acc: 0.247475. Loss: 2.190253. Batch_acc: 0.280069. Batch_loss: 2.110995 \n",
      "Batch: 1130. Acc: 0.247518. Loss: 2.190085. Batch_acc: 0.295313. Batch_loss: 2.004538 \n",
      "Batch: 1131. Acc: 0.247541. Loss: 2.189998. Batch_acc: 0.273256. Batch_loss: 2.090361 \n",
      "Batch: 1132. Acc: 0.247571. Loss: 2.189884. Batch_acc: 0.281918. Batch_loss: 2.060263 \n",
      "Batch: 1133. Acc: 0.247602. Loss: 2.189762. Batch_acc: 0.283626. Batch_loss: 2.049006 \n",
      "Batch: 1134. Acc: 0.247636. Loss: 2.189666. Batch_acc: 0.285880. Batch_loss: 2.080030 \n",
      "Batch: 1135. Acc: 0.247672. Loss: 2.189539. Batch_acc: 0.287896. Batch_loss: 2.048329 \n",
      "Batch: 1136. Acc: 0.247709. Loss: 2.189420. Batch_acc: 0.290063. Batch_loss: 2.054237 \n",
      "Batch: 1137. Acc: 0.247760. Loss: 2.189275. Batch_acc: 0.302732. Batch_loss: 2.032039 \n",
      "Batch: 1138. Acc: 0.247782. Loss: 2.189189. Batch_acc: 0.272727. Batch_loss: 2.090786 \n",
      "Batch: 1139. Acc: 0.247805. Loss: 2.189087. Batch_acc: 0.274713. Batch_loss: 2.073880 \n",
      "Batch: 1140. Acc: 0.247827. Loss: 2.188973. Batch_acc: 0.272156. Batch_loss: 2.059708 \n",
      "Batch: 1141. Acc: 0.247853. Loss: 2.188891. Batch_acc: 0.278669. Batch_loss: 2.091351 \n",
      "Batch: 1142. Acc: 0.247881. Loss: 2.188772. Batch_acc: 0.279885. Batch_loss: 2.053984 \n",
      "Batch: 1143. Acc: 0.247897. Loss: 2.188695. Batch_acc: 0.266055. Batch_loss: 2.101073 \n",
      "Batch: 1144. Acc: 0.247929. Loss: 2.188584. Batch_acc: 0.284404. Batch_loss: 2.061677 \n",
      "Batch: 1145. Acc: 0.247974. Loss: 2.188458. Batch_acc: 0.298196. Batch_loss: 2.046883 \n",
      "Batch: 1146. Acc: 0.247989. Loss: 2.188401. Batch_acc: 0.265306. Batch_loss: 2.121818 \n",
      "Batch: 1147. Acc: 0.248018. Loss: 2.188309. Batch_acc: 0.281503. Batch_loss: 2.083258 \n",
      "Batch: 1148. Acc: 0.248040. Loss: 2.188212. Batch_acc: 0.274259. Batch_loss: 2.075766 \n",
      "Batch: 1149. Acc: 0.248053. Loss: 2.188117. Batch_acc: 0.262192. Batch_loss: 2.078489 \n",
      "Batch: 1150. Acc: 0.248088. Loss: 2.188013. Batch_acc: 0.288087. Batch_loss: 2.069216 \n",
      "Batch: 1151. Acc: 0.248122. Loss: 2.187891. Batch_acc: 0.286356. Batch_loss: 2.051439 \n",
      "Batch: 1152. Acc: 0.248155. Loss: 2.187784. Batch_acc: 0.285633. Batch_loss: 2.065134 \n",
      "Batch: 1153. Acc: 0.248184. Loss: 2.187685. Batch_acc: 0.282876. Batch_loss: 2.072277 \n",
      "Batch: 1154. Acc: 0.248218. Loss: 2.187555. Batch_acc: 0.286279. Batch_loss: 2.039973 \n",
      "Batch: 1155. Acc: 0.248236. Loss: 2.187451. Batch_acc: 0.269120. Batch_loss: 2.067643 \n",
      "Batch: 1156. Acc: 0.248279. Loss: 2.187334. Batch_acc: 0.297680. Batch_loss: 2.053565 \n",
      "Batch: 1157. Acc: 0.248316. Loss: 2.187230. Batch_acc: 0.291375. Batch_loss: 2.065768 \n",
      "Batch: 1158. Acc: 0.248346. Loss: 2.187123. Batch_acc: 0.283171. Batch_loss: 2.063786 \n",
      "Batch: 1159. Acc: 0.248362. Loss: 2.187035. Batch_acc: 0.267531. Batch_loss: 2.082046 \n",
      "Batch: 1160. Acc: 0.248405. Loss: 2.186903. Batch_acc: 0.296400. Batch_loss: 2.037110 \n",
      "Batch: 1161. Acc: 0.248439. Loss: 2.186786. Batch_acc: 0.288078. Batch_loss: 2.052771 \n",
      "Batch: 1162. Acc: 0.248469. Loss: 2.186686. Batch_acc: 0.282167. Batch_loss: 2.072840 \n",
      "Batch: 1163. Acc: 0.248489. Loss: 2.186602. Batch_acc: 0.271986. Batch_loss: 2.087283 \n",
      "Batch: 1164. Acc: 0.248524. Loss: 2.186482. Batch_acc: 0.290493. Batch_loss: 2.044078 \n",
      "Batch: 1165. Acc: 0.248562. Loss: 2.186355. Batch_acc: 0.292994. Batch_loss: 2.037053 \n",
      "Batch: 1166. Acc: 0.248594. Loss: 2.186239. Batch_acc: 0.286211. Batch_loss: 2.050750 \n",
      "Batch: 1167. Acc: 0.248634. Loss: 2.186137. Batch_acc: 0.296060. Batch_loss: 2.065600 \n",
      "Batch: 1168. Acc: 0.248675. Loss: 2.186009. Batch_acc: 0.296404. Batch_loss: 2.036058 \n",
      "Batch: 1169. Acc: 0.248691. Loss: 2.185939. Batch_acc: 0.268293. Batch_loss: 2.102751 \n",
      "Batch: 1170. Acc: 0.248718. Loss: 2.185851. Batch_acc: 0.280630. Batch_loss: 2.082079 \n",
      "Batch: 1171. Acc: 0.248740. Loss: 2.185766. Batch_acc: 0.275029. Batch_loss: 2.083310 \n",
      "Batch: 1172. Acc: 0.248769. Loss: 2.185667. Batch_acc: 0.281993. Batch_loss: 2.071300 \n",
      "Batch: 1173. Acc: 0.248795. Loss: 2.185601. Batch_acc: 0.279588. Batch_loss: 2.108952 \n",
      "Batch: 1174. Acc: 0.248815. Loss: 2.185515. Batch_acc: 0.271739. Batch_loss: 2.085355 \n",
      "Batch: 1175. Acc: 0.248852. Loss: 2.185392. Batch_acc: 0.292683. Batch_loss: 2.038822 \n",
      "Batch: 1176. Acc: 0.248872. Loss: 2.185325. Batch_acc: 0.271940. Batch_loss: 2.106148 \n",
      "Batch: 1177. Acc: 0.248896. Loss: 2.185253. Batch_acc: 0.279635. Batch_loss: 2.096763 \n",
      "Batch: 1178. Acc: 0.248916. Loss: 2.185162. Batch_acc: 0.272245. Batch_loss: 2.074529 \n",
      "Batch: 1179. Acc: 0.248941. Loss: 2.185064. Batch_acc: 0.279164. Batch_loss: 2.068819 \n",
      "Batch: 1180. Acc: 0.248966. Loss: 2.185006. Batch_acc: 0.279138. Batch_loss: 2.115440 \n",
      "Batch: 1181. Acc: 0.249018. Loss: 2.184875. Batch_acc: 0.307018. Batch_loss: 2.037367 \n",
      "Batch: 1182. Acc: 0.249051. Loss: 2.184762. Batch_acc: 0.287774. Batch_loss: 2.051745 \n",
      "Batch: 1183. Acc: 0.249082. Loss: 2.184676. Batch_acc: 0.286203. Batch_loss: 2.083452 \n",
      "Batch: 1184. Acc: 0.249093. Loss: 2.184605. Batch_acc: 0.261688. Batch_loss: 2.101726 \n",
      "Batch: 1185. Acc: 0.249104. Loss: 2.184542. Batch_acc: 0.262908. Batch_loss: 2.107016 \n",
      "Batch: 1186. Acc: 0.249128. Loss: 2.184457. Batch_acc: 0.277306. Batch_loss: 2.085502 \n",
      "Batch: 1187. Acc: 0.249166. Loss: 2.184343. Batch_acc: 0.294664. Batch_loss: 2.047402 \n",
      "Batch: 1188. Acc: 0.249194. Loss: 2.184250. Batch_acc: 0.282609. Batch_loss: 2.072345 \n",
      "Batch: 1189. Acc: 0.249218. Loss: 2.184160. Batch_acc: 0.277364. Batch_loss: 2.077173 \n",
      "Batch: 1190. Acc: 0.249243. Loss: 2.184053. Batch_acc: 0.279632. Batch_loss: 2.056235 \n",
      "Batch: 1191. Acc: 0.249270. Loss: 2.183961. Batch_acc: 0.281268. Batch_loss: 2.074503 \n",
      "Batch: 1192. Acc: 0.249302. Loss: 2.183865. Batch_acc: 0.287888. Batch_loss: 2.067524 \n",
      "Batch: 1193. Acc: 0.249327. Loss: 2.183771. Batch_acc: 0.279883. Batch_loss: 2.070155 \n",
      "Batch: 1194. Acc: 0.249353. Loss: 2.183678. Batch_acc: 0.280236. Batch_loss: 2.070307 \n",
      "Batch: 1195. Acc: 0.249379. Loss: 2.183582. Batch_acc: 0.281824. Batch_loss: 2.065253 \n",
      "Batch: 1196. Acc: 0.249407. Loss: 2.183474. Batch_acc: 0.282869. Batch_loss: 2.055266 \n",
      "Batch: 1197. Acc: 0.249437. Loss: 2.183383. Batch_acc: 0.285217. Batch_loss: 2.074623 \n",
      "Batch: 1198. Acc: 0.249469. Loss: 2.183282. Batch_acc: 0.287174. Batch_loss: 2.063754 \n",
      "Batch: 1199. Acc: 0.249496. Loss: 2.183194. Batch_acc: 0.282051. Batch_loss: 2.076034 \n",
      "Batch: 1200. Acc: 0.249512. Loss: 2.183117. Batch_acc: 0.269209. Batch_loss: 2.090039 \n",
      "Batch: 1201. Acc: 0.249539. Loss: 2.183034. Batch_acc: 0.282787. Batch_loss: 2.082248 \n",
      "Batch: 1202. Acc: 0.249549. Loss: 2.182971. Batch_acc: 0.260920. Batch_loss: 2.105724 \n",
      "Batch: 1203. Acc: 0.249575. Loss: 2.182863. Batch_acc: 0.281087. Batch_loss: 2.052831 \n",
      "Batch: 1204. Acc: 0.249601. Loss: 2.182753. Batch_acc: 0.280543. Batch_loss: 2.052657 \n",
      "Batch: 1205. Acc: 0.249636. Loss: 2.182649. Batch_acc: 0.292442. Batch_loss: 2.056207 \n",
      "Batch: 1206. Acc: 0.249662. Loss: 2.182558. Batch_acc: 0.280411. Batch_loss: 2.072872 \n",
      "Batch: 1207. Acc: 0.249691. Loss: 2.182450. Batch_acc: 0.285548. Batch_loss: 2.051267 \n",
      "Batch: 1208. Acc: 0.249721. Loss: 2.182346. Batch_acc: 0.286288. Batch_loss: 2.057716 \n",
      "Batch: 1209. Acc: 0.249740. Loss: 2.182251. Batch_acc: 0.272163. Batch_loss: 2.068807 \n",
      "Batch: 1210. Acc: 0.249764. Loss: 2.182144. Batch_acc: 0.278641. Batch_loss: 2.052880 \n",
      "Batch: 1211. Acc: 0.249799. Loss: 2.182034. Batch_acc: 0.291908. Batch_loss: 2.048786 \n",
      "Batch: 1212. Acc: 0.249821. Loss: 2.181932. Batch_acc: 0.276256. Batch_loss: 2.058687 \n",
      "Batch: 1213. Acc: 0.249845. Loss: 2.181836. Batch_acc: 0.279906. Batch_loss: 2.062284 \n",
      "Batch: 1214. Acc: 0.249869. Loss: 2.181771. Batch_acc: 0.279499. Batch_loss: 2.100622 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 1215. Acc: 0.249886. Loss: 2.181677. Batch_acc: 0.272033. Batch_loss: 2.065346 \n",
      "Batch: 1216. Acc: 0.249925. Loss: 2.181564. Batch_acc: 0.296045. Batch_loss: 2.046174 \n",
      "Batch: 1217. Acc: 0.249956. Loss: 2.181464. Batch_acc: 0.286678. Batch_loss: 2.062533 \n",
      "Batch: 1218. Acc: 0.249989. Loss: 2.181354. Batch_acc: 0.290379. Batch_loss: 2.045523 \n",
      "Batch: 1219. Acc: 0.250010. Loss: 2.181256. Batch_acc: 0.276081. Batch_loss: 2.061839 \n",
      "Batch: 1220. Acc: 0.250047. Loss: 2.181132. Batch_acc: 0.294785. Batch_loss: 2.032133 \n",
      "Batch: 1221. Acc: 0.250070. Loss: 2.181055. Batch_acc: 0.277939. Batch_loss: 2.087189 \n",
      "Batch: 1222. Acc: 0.250095. Loss: 2.180981. Batch_acc: 0.280783. Batch_loss: 2.090099 \n",
      "Batch: 1223. Acc: 0.250125. Loss: 2.180918. Batch_acc: 0.286942. Batch_loss: 2.103530 \n",
      "Batch: 1224. Acc: 0.250161. Loss: 2.180822. Batch_acc: 0.294529. Batch_loss: 2.062601 \n",
      "Batch: 1225. Acc: 0.250198. Loss: 2.180723. Batch_acc: 0.295195. Batch_loss: 2.060216 \n",
      "Batch: 1226. Acc: 0.250220. Loss: 2.180647. Batch_acc: 0.277875. Batch_loss: 2.086475 \n",
      "Batch: 1227. Acc: 0.250228. Loss: 2.180588. Batch_acc: 0.259572. Batch_loss: 2.108857 \n",
      "Batch: 1228. Acc: 0.250264. Loss: 2.180470. Batch_acc: 0.294218. Batch_loss: 2.038176 \n",
      "Batch: 1229. Acc: 0.250289. Loss: 2.180369. Batch_acc: 0.280000. Batch_loss: 2.058672 \n",
      "Batch: 1230. Acc: 0.250330. Loss: 2.180268. Batch_acc: 0.301219. Batch_loss: 2.055648 \n",
      "Batch: 1231. Acc: 0.250364. Loss: 2.180152. Batch_acc: 0.291833. Batch_loss: 2.038200 \n",
      "Batch: 1232. Acc: 0.250384. Loss: 2.180067. Batch_acc: 0.276545. Batch_loss: 2.070834 \n",
      "Batch: 1233. Acc: 0.250393. Loss: 2.179996. Batch_acc: 0.261350. Batch_loss: 2.091362 \n",
      "Batch: 1234. Acc: 0.250411. Loss: 2.179920. Batch_acc: 0.271884. Batch_loss: 2.086010 \n",
      "Batch: 1235. Acc: 0.250435. Loss: 2.179841. Batch_acc: 0.280854. Batch_loss: 2.081485 \n",
      "Batch: 1236. Acc: 0.250466. Loss: 2.179725. Batch_acc: 0.288361. Batch_loss: 2.035500 \n",
      "Batch: 1237. Acc: 0.250505. Loss: 2.179622. Batch_acc: 0.300716. Batch_loss: 2.047860 \n",
      "Batch: 1238. Acc: 0.250539. Loss: 2.179523. Batch_acc: 0.293083. Batch_loss: 2.054823 \n",
      "Batch: 1239. Acc: 0.250578. Loss: 2.179416. Batch_acc: 0.299713. Batch_loss: 2.046596 \n",
      "Batch: 1240. Acc: 0.250593. Loss: 2.179344. Batch_acc: 0.269231. Batch_loss: 2.091297 \n",
      "Batch: 1241. Acc: 0.250625. Loss: 2.179259. Batch_acc: 0.289937. Batch_loss: 2.074487 \n",
      "Batch: 1242. Acc: 0.250646. Loss: 2.179184. Batch_acc: 0.276596. Batch_loss: 2.086183 \n",
      "Batch: 1243. Acc: 0.250664. Loss: 2.179116. Batch_acc: 0.272779. Batch_loss: 2.094919 \n",
      "Batch: 1244. Acc: 0.250689. Loss: 2.179031. Batch_acc: 0.282816. Batch_loss: 2.069681 \n",
      "Batch: 1245. Acc: 0.250712. Loss: 2.178950. Batch_acc: 0.278975. Batch_loss: 2.076159 \n",
      "Batch: 1246. Acc: 0.250737. Loss: 2.178855. Batch_acc: 0.282460. Batch_loss: 2.061592 \n",
      "Batch: 1247. Acc: 0.250768. Loss: 2.178771. Batch_acc: 0.289889. Batch_loss: 2.073044 \n",
      "Batch: 1248. Acc: 0.250791. Loss: 2.178663. Batch_acc: 0.278726. Batch_loss: 2.045757 \n",
      "Batch: 1249. Acc: 0.250811. Loss: 2.178579. Batch_acc: 0.275362. Batch_loss: 2.077002 \n",
      "Batch: 1250. Acc: 0.250840. Loss: 2.178457. Batch_acc: 0.287042. Batch_loss: 2.023643 \n",
      "Batch: 1251. Acc: 0.250862. Loss: 2.178381. Batch_acc: 0.279112. Batch_loss: 2.079088 \n",
      "Batch: 1252. Acc: 0.250898. Loss: 2.178263. Batch_acc: 0.296465. Batch_loss: 2.032277 \n",
      "Batch: 1253. Acc: 0.250921. Loss: 2.178155. Batch_acc: 0.279018. Batch_loss: 2.046569 \n",
      "Batch: 1254. Acc: 0.250954. Loss: 2.178034. Batch_acc: 0.290981. Batch_loss: 2.029474 \n",
      "Batch: 1255. Acc: 0.250993. Loss: 2.177912. Batch_acc: 0.300000. Batch_loss: 2.026789 \n",
      "Checkpointing on batch: 1255. Accuracy: 0.250993321343063. Loss per char: 2.177912326898527. Time: 1627203534.6225615\n",
      "Last question is tensor([ 2, 49, 86, 85,  1, 85, 80, 72, 70, 85, 73, 70, 83,  1, 22, 23,  1, 66,\n",
      "        79, 69,  1, 14, 19, 25, 24, 17, 26, 26, 20, 25, 18, 21, 19, 26, 15,  3,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0], device='cuda:0')\n",
      "Removing existing model file at checkpoints\\add_or_sub-2021-07-25_latest_checkpoint.pth\n",
      "Starting checkpoint save of checkpoints\\add_or_sub-2021-07-25_latest_checkpoint.pth...\n",
      "Final saved model size: 530790651\n",
      "Batch: 1256. Acc: 0.251017. Loss: 2.177806. Batch_acc: 0.280384. Batch_loss: 2.046587 \n",
      "Batch: 1257. Acc: 0.251049. Loss: 2.177712. Batch_acc: 0.290638. Batch_loss: 2.059625 \n",
      "Batch: 1258. Acc: 0.251065. Loss: 2.177626. Batch_acc: 0.272245. Batch_loss: 2.067682 \n",
      "Batch: 1259. Acc: 0.251089. Loss: 2.177540. Batch_acc: 0.281034. Batch_loss: 2.069136 \n",
      "Batch: 1260. Acc: 0.251112. Loss: 2.177472. Batch_acc: 0.281158. Batch_loss: 2.089631 \n",
      "Batch: 1261. Acc: 0.251135. Loss: 2.177385. Batch_acc: 0.278927. Batch_loss: 2.070974 \n",
      "Batch: 1262. Acc: 0.251162. Loss: 2.177299. Batch_acc: 0.285965. Batch_loss: 2.066606 \n",
      "Batch: 1263. Acc: 0.251205. Loss: 2.177179. Batch_acc: 0.304471. Batch_loss: 2.027985 \n",
      "Batch: 1264. Acc: 0.251238. Loss: 2.177091. Batch_acc: 0.292994. Batch_loss: 2.065202 \n",
      "Batch: 1265. Acc: 0.251268. Loss: 2.176988. Batch_acc: 0.288914. Batch_loss: 2.047339 \n",
      "Batch: 1266. Acc: 0.251290. Loss: 2.176907. Batch_acc: 0.279907. Batch_loss: 2.073652 \n",
      "Batch: 1267. Acc: 0.251308. Loss: 2.176842. Batch_acc: 0.275676. Batch_loss: 2.089696 \n",
      "Batch: 1268. Acc: 0.251341. Loss: 2.176733. Batch_acc: 0.292808. Batch_loss: 2.040102 \n",
      "Batch: 1269. Acc: 0.251377. Loss: 2.176632. Batch_acc: 0.297001. Batch_loss: 2.047777 \n",
      "Batch: 1270. Acc: 0.251402. Loss: 2.176547. Batch_acc: 0.283943. Batch_loss: 2.066054 \n",
      "Batch: 1271. Acc: 0.251434. Loss: 2.176424. Batch_acc: 0.291525. Batch_loss: 2.022751 \n",
      "Batch: 1272. Acc: 0.251452. Loss: 2.176380. Batch_acc: 0.275000. Batch_loss: 2.118995 \n",
      "Batch: 1273. Acc: 0.251475. Loss: 2.176279. Batch_acc: 0.279724. Batch_loss: 2.047673 \n",
      "Batch: 1274. Acc: 0.251499. Loss: 2.176195. Batch_acc: 0.281875. Batch_loss: 2.069968 \n",
      "Batch: 1275. Acc: 0.251520. Loss: 2.176131. Batch_acc: 0.279056. Batch_loss: 2.095368 \n",
      "Batch: 1276. Acc: 0.251551. Loss: 2.176043. Batch_acc: 0.291074. Batch_loss: 2.064507 \n",
      "Batch: 1277. Acc: 0.251576. Loss: 2.175953. Batch_acc: 0.282659. Batch_loss: 2.061021 \n",
      "Batch: 1278. Acc: 0.251598. Loss: 2.175857. Batch_acc: 0.279239. Batch_loss: 2.056808 \n",
      "Batch: 1279. Acc: 0.251614. Loss: 2.175766. Batch_acc: 0.272059. Batch_loss: 2.061182 \n",
      "Batch: 1280. Acc: 0.251641. Loss: 2.175665. Batch_acc: 0.286464. Batch_loss: 2.043977 \n",
      "Batch: 1281. Acc: 0.251662. Loss: 2.175581. Batch_acc: 0.278596. Batch_loss: 2.069695 \n",
      "Batch: 1282. Acc: 0.251690. Loss: 2.175502. Batch_acc: 0.285954. Batch_loss: 2.076938 \n",
      "Batch: 1283. Acc: 0.251727. Loss: 2.175391. Batch_acc: 0.299597. Batch_loss: 2.033601 \n",
      "Batch: 1284. Acc: 0.251738. Loss: 2.175328. Batch_acc: 0.266204. Batch_loss: 2.094310 \n",
      "Batch: 1285. Acc: 0.251760. Loss: 2.175242. Batch_acc: 0.278771. Batch_loss: 2.067292 \n",
      "Batch: 1286. Acc: 0.251779. Loss: 2.175171. Batch_acc: 0.277158. Batch_loss: 2.082604 \n",
      "Batch: 1287. Acc: 0.251804. Loss: 2.175108. Batch_acc: 0.283140. Batch_loss: 2.092938 \n",
      "Batch: 1288. Acc: 0.251830. Loss: 2.175026. Batch_acc: 0.284989. Batch_loss: 2.071815 \n",
      "Batch: 1289. Acc: 0.251850. Loss: 2.174925. Batch_acc: 0.276993. Batch_loss: 2.046098 \n",
      "Batch: 1290. Acc: 0.251876. Loss: 2.174833. Batch_acc: 0.285386. Batch_loss: 2.057076 \n",
      "Batch: 1291. Acc: 0.251900. Loss: 2.174735. Batch_acc: 0.284539. Batch_loss: 2.045680 \n",
      "Batch: 1292. Acc: 0.251936. Loss: 2.174635. Batch_acc: 0.300000. Batch_loss: 2.040415 \n",
      "Batch: 1293. Acc: 0.251969. Loss: 2.174526. Batch_acc: 0.293191. Batch_loss: 2.037192 \n",
      "Batch: 1294. Acc: 0.251998. Loss: 2.174428. Batch_acc: 0.288385. Batch_loss: 2.049253 \n",
      "Batch: 1295. Acc: 0.252020. Loss: 2.174326. Batch_acc: 0.280732. Batch_loss: 2.043277 \n",
      "Batch: 1296. Acc: 0.252032. Loss: 2.174233. Batch_acc: 0.267929. Batch_loss: 2.054087 \n",
      "Batch: 1297. Acc: 0.252052. Loss: 2.174135. Batch_acc: 0.278660. Batch_loss: 2.043340 \n",
      "Batch: 1298. Acc: 0.252074. Loss: 2.174062. Batch_acc: 0.280618. Batch_loss: 2.076484 \n",
      "Batch: 1299. Acc: 0.252100. Loss: 2.173975. Batch_acc: 0.286947. Batch_loss: 2.060983 \n",
      "Batch: 1300. Acc: 0.252129. Loss: 2.173873. Batch_acc: 0.288549. Batch_loss: 2.044086 \n",
      "Batch: 1301. Acc: 0.252145. Loss: 2.173797. Batch_acc: 0.273460. Batch_loss: 2.074803 \n",
      "Batch: 1302. Acc: 0.252183. Loss: 2.173683. Batch_acc: 0.300971. Batch_loss: 2.026203 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 1303. Acc: 0.252205. Loss: 2.173587. Batch_acc: 0.281287. Batch_loss: 2.046254 \n",
      "Batch: 1304. Acc: 0.252238. Loss: 2.173498. Batch_acc: 0.295612. Batch_loss: 2.057472 \n",
      "Batch: 1305. Acc: 0.252267. Loss: 2.173399. Batch_acc: 0.288926. Batch_loss: 2.046735 \n",
      "Batch: 1306. Acc: 0.252303. Loss: 2.173296. Batch_acc: 0.299652. Batch_loss: 2.037601 \n",
      "Batch: 1307. Acc: 0.252326. Loss: 2.173203. Batch_acc: 0.283063. Batch_loss: 2.050318 \n"
     ]
    }
   ],
   "source": [
    "model_process.train(\n",
    "    name = exp_name +\"-\" + unique_id,\n",
    "    model = model,\n",
    "    training_data= train_loader,\n",
    "    validation_data = val_loader,\n",
    "    interpolate_data=interpolate_loader,\n",
    "    optimizer = optimizer,\n",
    "    device = device,\n",
    "    epochs=8,\n",
    "    tb=None,\n",
    "    log_interval=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Training from Tensorboard data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Restore best model for this experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# build default transformer model\n",
    "model = utils.build_transformer()\n",
    "\n",
    "#model_exp_name = \"linear_algebra\" # \"math_ds_algebra_linear_1d_easy\"\n",
    "#model_unique_id  = \"2020-07-22\" # \"2019-05-25_0900\"\n",
    "#model_exp_name = 'math_ds_algebra_linear_1d_easy'\n",
    "#model_unique_id = '2019-10-27_2300'\n",
    "# restore best validation model from checkpoint\n",
    "_ = checkpoints.restore_checkpoint(\".\\\\checkpoints\\\\checkpoint_b37504_e7.pth\",\"\", model=model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading tensorboard events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> As we can see, loss per char on validation dataset has a nice optimization curve but for interpolate, it's not the case. It's quite normal, interpolate contains more difficult and general cases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy Evolution during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [10, 6]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(\n",
    "    list(map(lambda l: l.step, valid_accuracy)),\n",
    "    list(map(lambda l: l.value, valid_accuracy)),\n",
    "    marker='+', label='Validation Accuracy'\n",
    ")\n",
    "ax.plot(\n",
    "    list(map(lambda l: l.step, interpolate_accuracy)),\n",
    "    list(map(lambda l: l.value, interpolate_accuracy)),\n",
    "    marker='+', label='Interpolate Accuracy'\n",
    ")\n",
    "plt.title('Algebra/Linear_1d Accuracy')\n",
    "ax.legend(loc='upper left', frameon=False)\n",
    "plt.xticks(np.arange(0, 20, step=1.0))\n",
    "plt.yticks(np.arange(0.3, 1.0, step=0.1))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Accuracy for validation dataset is growing constantly up to 85% while for interpolate dataset, it doesn't change much. Interpolate dataset contains too complicated and generic problems compared to training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_process.predict_single(\"Solve 5*w + 3 = -2 for w.\", model, device, n_best=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_process.predict_single(\"Solve 212 = 56*z - 12 for z.\", model, device, n_best=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_process.predict_single(\"Solve 2514*m = 2508*m - 24 for m.\", model, device, n_best=1)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "mathematics_dataset_transformer.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "interpreter": {
   "hash": "f2af42a63cc7d68e489f56292c3c9bd50be64ca32158c5e1ba92e8c017e2a89f"
  },
  "kernelspec": {
   "display_name": "Python (torch)",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
